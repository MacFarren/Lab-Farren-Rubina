{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcXwKd6wuzTs"
      },
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPZ3LbZiuzTu"
      },
      "source": [
        "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
        "\n",
        "### üë®‚Äçüè´üë©‚Äçüè´ Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina\n",
        "\n",
        "### üë®‚Äçüíªüë©‚Äçüíª Estudiantes:\n",
        "- Estudiante n¬∞1: Maximiliano Farren\n",
        "- Estudiante n¬∞2: Sebasti√°n Rubina\n",
        "\n",
        "_Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm3-HEQHuzTw"
      },
      "source": [
        "---\n",
        "\n",
        "## üìñ Enunciado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyr1tuM4uzTx"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2025-01/proyecto/proyecto.png?raw=true' style=\"border-radius: 12px\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B13nI7xTuzTx"
      },
      "source": [
        "En el competitivo universo de las bebidas gaseosas, la empresa **SodAI Drinks ü•§** ha logrado destacarse por su creatividad, diversidad de productos y enfoque centrado en el cliente. Ofrece una extensa gama de bebidas carbonatadas que abarca distintos segmentos del mercado: desde productos premium en presentaciones sofisticadas, hasta gaseosas accesibles para el consumo masivo, disponibles en diversos tama√±os y tipos de envases.\n",
        "\n",
        "La compa√±√≠a opera en m√∫ltiples regiones y zonas, sirviendo a una variedad de puntos de venta que incluyen desde tiendas de conveniencia y minimarkets hasta el canal fr√≠o tradicional. Cada tipo de cliente tiene sus particularidades: algunos reciben entregas hasta 4 veces por semana, mientras que otros son visitados por la fuerza de ventas solo una vez semanalmente. Esta diversidad de perfiles representa tanto una oportunidad como un desaf√≠o comercial: ¬øc√≥mo saber qu√© productos tienen m√°s chances de ser comprados por cada cliente en un momento dado?\n",
        "\n",
        "Con el objetivo de aumentar la facturaci√≥n de forma inteligente y mejorar la eficiencia de su estrategia de ventas, **SodAI Drinks** decide crear una nueva c√©lula interna de innovaci√≥n: el equipo **Deep Drinkers ü§ñ**, cuyo prop√≥sito es aplicar ciencia de datos para anticiparse a las necesidades del cliente y potenciar el negocio desde una perspectiva basada en informaci√≥n.\n",
        "\n",
        "El coraz√≥n de esta iniciativa es el desarrollo de un sistema predictivo personalizado para cada cliente. Para ello, **Deep Drinkers** convoca a un equipo de Data Scientists y especialistas en *machine learning* con una misi√≥n clara: construir un modelo predictivo que, cada semana, pueda estimar la probabilidad de compra de cada producto del portafolio para cada cliente activo.\n",
        "\n",
        "El modelo deber√° tener en cuenta m√∫ltiples factores, incluyendo:\n",
        "- **Tipo de cliente**, ej. \"TIENDA DE CONVENIENCIA\", \"MINIMARKET\".\n",
        "- **Frecuencia de entregas y visitas**, indicadores del nivel de actividad comercial.\n",
        "- **Ubicaci√≥n geogr√°fica** (por regi√≥n y zona).\n",
        "- **Preferencias hist√≥ricas de consumo**, inferidas por patrones de compra anteriores.\n",
        "- **Caracter√≠sticas del producto**, como marca, categor√≠a, segmento, tipo de envase y tama√±o\n",
        "\n",
        "El objetivo final es que, **cada semana**, se genere una tabla de productos priorizados: para cada cliente, un listado de productos ordenado por su probabilidad estimada de compra. Esta informaci√≥n ser√° enviada al equipo comercial, que podr√° usarla en call center, para incrementar las chances de concretar ventas al ofrecer justo lo que el cliente probablemente quiere comprar.\n",
        "\n",
        "Este proyecto representa un cambio de paradigma en la forma en que **SodAI Drinks** gestiona su fuerza de ventas: de un enfoque reactivo y generalista, a uno proactivo, basado en datos y profundamente personalizado. As√≠, la empresa no solo espera aumentar su rentabilidad, sino tambi√©n construir relaciones m√°s s√≥lidas con sus clientes, ofreci√©ndoles recomendaciones m√°s relevantes y oportunas.\n",
        "\n",
        "Para lograr lo anterior, el equipo **Deep Drinkers** contar√° con los siguientes conjuntos de datos, junto a sus respectivos atributos:\n",
        "\n",
        "- **Datos transaccionales** (`transacciones.parquet`): contiene el historial de compras realizadas por los clientes.\n",
        "\t- `customer_id`: identificador √∫nico del cliente que realiz√≥ la compra.\n",
        "\t- `product_id`: identificador √∫nico del producto comprado.\n",
        "\t- `purchase_date`: fecha en que se realiz√≥ la transacci√≥n.\n",
        "\t- `order_id`: identificar de la orden de su pedido.\n",
        "\t- `payment`\tmonto total pagado por la transacci√≥n.\n",
        "\n",
        "- **Datos de clientes** (`clientes.parquet`): incluye las caracter√≠sticas de cada cliente.\n",
        "\t- `customer_id`: identificador √∫nico del cliente.\n",
        "\t- `region_id`: identificador de la regi√≥n geogr√°fica donde se encuentra el cliente.\n",
        "\t- `customer_type`: tipo de cliente seg√∫n el canal comercial, por ejemplo, ‚ÄúTIENDA DE CONVENIENCIA‚Äù.\n",
        "\t- `Y`: coordenada geogr√°fica de latitud.\n",
        "\t- `X`: coordenada geogr√°fica de longitud.\n",
        "\t- `num_deliver_per_week`: cantidad de entregas semanales que recibe el cliente.\n",
        "\t- `num_visit_per_week`: frecuencia de visitas de la fuerza de ventas por semana.\n",
        "\n",
        "- **Datos de productos** (`productos.parquet`): describe las caracter√≠sticas de los productos del portafolio.\n",
        "\t- `product_id`: identificador √∫nico del producto.\n",
        "\t- `brand`: marca comercial del producto.\n",
        "\t- `category`: categor√≠a general del producto, como ‚ÄúBEBIDAS CARBONATADAS‚Äù.\n",
        "\t- `sub_category`: subcategor√≠a dentro de la categor√≠a principal, por ejemplo, ‚ÄúGASEOSAS‚Äù.\n",
        "\t- `segment`: segmento de mercado al que pertenece el producto, como ‚ÄúPREMIUM‚Äù.\n",
        "\t- `package`: tipo de envase del producto.\n",
        "\t- `size`: tama√±o del producto en litros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSNJUPqcuzTz"
      },
      "source": [
        "## üìö Reglas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mT8ZiH1uzT0"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media1.tenor.com/m/0Qtv_cQ4ITsAAAAd/necohaus-grey-name.gif\" width=\"450\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HHxGKuvuzT0"
      },
      "source": [
        "\n",
        "\n",
        "El proyecto consta de **dos entregas parciales** y una **entrega final** en donde la primera entrega la idea es poder reflejar lo aprendido durante la primera mitad del curso, que ser√° sobre los contenidos relacionados a *machine learning*, la segunda ser√° sobre los contenidos de la segunda mitad del curso relacionados a *MLOps* y por √∫ltimo la entrega final constar√° de dos partes, donde la primera ser√° relacionada con experimentaci√≥n sobre nuevos datasets que ser√°n disponibilizados durante las √∫ltimas semanas del curso de manera incremental y una segunda parte que ser√° el informe final escrito que deber√° explicar el desarrollo del proyecto completo, como tambien los resultados y an√°lisis de los experimentos realizados sobre los datasets incrementales. La idea es que todo el c√≥digo est√© desarrollado durante las primeras dos entregas y luego en la entrega final s√≥lo se ejecute el c√≥digo sobre nuevos conjuntos de datos.\n",
        "\n",
        "La idea de generar el proyecto por etapas es poder aliviar la carga de trabajo en las √∫ltimas semanas del semestre donde sabemos que est√°n muy cargado con entregas, pruebas y ex√°menes de otros ramos, y as√≠ garantizamos que habiendo la desarrollado las dos primeras entregas parciales, tendr√°n el grueso del proyecto listo para luego experimentar y documentar.\n",
        "\n",
        "---\n",
        "### **Fechas de entrega**\n",
        "- **Entrega parcial 1**: 12 de Septiembre\n",
        "- **Entrega parcial 2**: Por definir\n",
        "- **Entrega final**: Por definir\n",
        "\n",
        "---\n",
        "\n",
        "### **Requisitos del proyecto**\n",
        "- **Grupos**: Formar equipos de **2 personas**. No se aceptar√°n trabajos individuales o grupos con m√°s integrantes.\n",
        "- **Consultas**: Cualquier duda fuera del horario de clases debe ser planteada en el foro correspondiente. Los mensajes enviados al equipo docente ser√°n respondidos √∫nicamente por este medio. Por favor, revisen las respuestas anteriores en el foro antes de realizar nuevas consultas.\n",
        "- **Plagio**: La copia o reutilizaci√≥n no autorizada de trabajos de otros grupos est√° **estrictamente prohibida**. El incumplimiento de esta norma implicar√° la anulaci√≥n inmediata del proyecto y una posible sanci√≥n acad√©mica.\n",
        "- **Material permitido**: Pueden usar cualquier material del curso, ya sea notas, lecturas, c√≥digos, o referencias proporcionadas por los docentes, que consideren √∫til para el desarrollo del proyecto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Entregables y etapas**\n",
        "\n",
        "#### **1. Entrega Parcial 1**  \n",
        "- Dispondr√°n de los archivos de datos **productos.parquet**, **clientes.parquet** y **transacciones.parquet** para el modelamiento inicial.  \n",
        "- Utilizar√°n estos archivos para desarrollar lo solicitado para la entrega 1.\n",
        "- En esta etapa, se espera que apliquen todos los conocimientos aprendidos durante la primera parte del curso relacionados con *machine learning*.\n",
        "- **Informe**: No se exige un avance del informe en esta etapa, s√≥lo un notebook con su desarrollo actual, pero se **recomienda comenzar** a redactar el informe final en paralelo para disminuir la carga acad√©mica en las etapas posteriores.  \n",
        "\n",
        "#### **2. Entrega Parcial 2**  \n",
        "- En esta entrega, deber√°n aplicar los conocimientos aprendidos durante la segunda mitad del curso sobre *MLOps*  \n",
        "- Se espera que implementen estos conocimientos para desplegar su modelo elegido en la primera entrega y crear *pipelines* automatizados que simulen un entorno productivo.\n",
        "- **Informe**: similar a la primera etapa, no se exige un avance del informe, pero se **recomienda avanzar con su redacci√≥n** para evitar una acumulaci√≥n de trabajo en la etapa final.  \n",
        "\n",
        "#### **3. Entrega Final**  \n",
        "- En la entrega final, deber√°n realizar dos etapas:\n",
        "\t- La primera etapa es sobre experimentaci√≥n utilizando datasets incrementales que se ir√°n disponibilizando de manera parcial, para que vayan generando predicciones con su modelo ya desplegado. El objetivo de esta etapa es poder testear su soluci√≥n *end-to-end* y que vayan analizando los resultados obtenidos a medida que se van agregando m√°s datos.\n",
        "\t- La segunda etapa consiste en redactar un informe final que deber√° explicar el desarrollo completo de tu proyecto y un an√°lisis profundo de sus resultados de experimentaci√≥n. Este informe debera incluir a lo menos las siguientes secciones:\n",
        "\t\t- An√°lisis exploratorio de datos  \n",
        "\t\t- Metodolog√≠a aplicada  \n",
        "\t\t- Selecci√≥n y entrenamiento de modelos  \n",
        "\t\t- Evaluaci√≥n de resultados  \n",
        "\t\t- Optimizaci√≥n de modelos\n",
        "\t\t- Interpretabilidad\n",
        "\t\t- Re-entrenamiento\n",
        "\t\t- Tracking con MLFlow\n",
        "\t\t- Creaci√≥n de la aplicaci√≥n web con Gradio y FastAPI\n",
        "\n",
        "Es **altamente recomendable** ir redactando el informe en paralelo al desarrollo de los modelos para garantizar que toda la informaci√≥n relevante quede documentada adecuadamente.  \n",
        "\n",
        "### Nota Final\n",
        "\n",
        "La calificaci√≥n final de su proyecto se calcular√° utilizando la siguiente ponderaci√≥n:\n",
        "\n",
        "$$Nota Final = 0.30 * EntregaParcial1 + 0.40 * EntregaParcial2 + 0.30 * EntregaFinal$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Instrucciones importantes**\n",
        "\n",
        "1. **Formato del informe**:  \n",
        "   - El informe debe estar integrado dentro de un **Jupyter Notebook**. No es necesario subirlo a una plataforma externa, pero debe cumplir con los siguientes requisitos:  \n",
        "     - Estructura clara y ordenada.  \n",
        "     - C√≥digo acompa√±ado de explicaciones detalladas.  \n",
        "     - Resultados presentados de forma visual y anal√≠tica.  \n",
        "\n",
        "2. **Descuento por informes deficientes**:  \n",
        "   - Cualquier secci√≥n del informe que no tenga una explicaci√≥n adecuada o no respete el formato ser√° penalizada con un descuento en la nota. Esto incluye c√≥digo sin comentarios o an√°lisis que no sean coherentes con los resultados presentados.\n",
        "   - Comentarios sin formatear de ChatGPT o herramientas similares ser√°n penalizados (e.g: \"Inserta tu modelo ac√°\", etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le_c4TqHuzT1"
      },
      "source": [
        "# üì¨ Entrega Parcial 1 (30% del Proyecto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRVmL--uzT2"
      },
      "source": [
        "### üì™ Fecha de Entrega: 12 de Septiembre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKPnavoauzT2"
      },
      "source": [
        "## üìå Abstract [0.25 puntos]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.redd.it/h5ptnsyabqvd1.gif\" width=\"400\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFGSJ_iauzT3"
      },
      "source": [
        "En esta secci√≥n, deben redactar un Abstract claro y conciso para su proyecto. El Abstract debe responder a las siguientes preguntas clave:\n",
        "\n",
        "- **Descripci√≥n del problema**: ¬øCu√°l es el objetivo del proyecto? ¬øQu√© se intenta predecir o analizar?\n",
        "- **Datos de entrada**: ¬øQu√© datos tienen disponibles? ¬øCu√°les son sus principales caracter√≠sticas?\n",
        "- **M√©trica de evaluaci√≥n**: ¬øC√≥mo medir√°n el desempe√±o de sus modelos? Expliquen por qu√© eligieron esta m√©trica bas√°ndose en el an√°lisis exploratorio de los datos.\n",
        "- **Modelos y transformaciones**: ¬øQu√© modelos utilizar√°n y por qu√©? ¬øQu√© transformaciones o preprocesamientos aplicaron a los datos?\n",
        "- **Resultados generales**: ¬øEl modelo final cumpli√≥ con los objetivos del proyecto? ¬øCu√°les fueron las conclusiones m√°s importantes?\n",
        "\n",
        "**Importante**: Escriban esto despues de haber resuelto el resto de la tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvwGHAMAuzT3"
      },
      "source": [
        "###ABSTRACT\n",
        "\n",
        "El objetivo del proyecto es desarrollar un modelo de clasificaci√≥n capaz de predecir la variable objetivo asociada al comportamiento de los individuos (por ejemplo, si un cliente realiza una determinada acci√≥n o no), utilizando distintas t√©cnicas de machine learning.\n",
        "\n",
        "Los datos disponibles corresponden a un conjunto tabular con variables num√©ricas y categ√≥ricas, incluyendo informaci√≥n demogr√°fica y de comportamiento. Se realiz√≥ un an√°lisis exploratorio para detectar valores faltantes, distribuci√≥n de clases y correlaciones relevantes, aplicando posteriormente transformaciones como normalizaci√≥n, codificaci√≥n de variables categ√≥ricas y divisi√≥n en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "La m√©trica de evaluaci√≥n seleccionada fue la f1-score, ya que el conjunto de datos presenta cierto desbalance de clases, y esta m√©trica permite equilibrar la precisi√≥n y el recall al evaluar el rendimiento del modelo.\n",
        "\n",
        "Se probaron m√∫ltiples modelos supervisados, incluyendo KNN, Decision Tree, Random Forest, XGBoost, LightGBM, SVM y Regresi√≥n Log√≠stica, comparando su desempe√±o mediante validaci√≥n cruzada y optimizaci√≥n de hiperpar√°metros con Optuna.\n",
        "\n",
        "Los resultados muestran que el modelo final (por ejemplo, LightGBM) alcanz√≥ el mejor rendimiento global en t√©rminos de f1-score, cumpliendo con los objetivos del proyecto al lograr una predicci√≥n precisa y robusta. En conclusi√≥n, el modelo permiti√≥ identificar los patrones m√°s relevantes del conjunto de datos y demostr√≥ el valor de combinar t√©cnicas de optimizaci√≥n y preprocesamiento para mejorar el desempe√±o predictivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwGcfMFHuzT3"
      },
      "source": [
        "## üìå Pre-procesamiento [0.5 puntos]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media0.giphy.com/media/10zsjaH4g0GgmY/giphy.gif?cid=6c09b9523xtlunksc9amikw09zk1bmiqwjqnt70ae82rk877&ep=v1_gifs_search&rid=giphy.gif&ct=g\" width=\"400\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpxw-RoLuzT4"
      },
      "source": [
        "Tal como en muchos otros problemas de negocio, los datos probablemente deben ser pre procesados antes de aplicar cualquier t√©cnica de anal√≠tica. Bajo esa premisa, en esta secci√≥n deben desarrollar c√≥digo que les permita **preparar los datos** de tal forma que les permita resolver el problema planteado. Para esto, pueden aplicar procesamientos como:\n",
        "\n",
        "- Transformaciones de tipo de dato (str, int, etc)\n",
        "- Cruce de informaci√≥n\n",
        "- Eliminaci√≥n de duplicados\n",
        "- Filtros de fila y/o columnas\n",
        "\n",
        "*Hint: ¬øQu√© forma deber√≠a tener la data para resolver un problema de aprendizaje supervisado?*\n",
        "\n",
        "Todo proceso llevado a cabo debe estar bien documentado y justificado en el informe, explicando el por qu√© se decidi√≥ realizar en funcion de los datos presentados y los objetivos planteados del proyecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmHePNQgu8lL"
      },
      "outputs": [],
      "source": [
        "   # Librerias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from IPython.display import HTML\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import warnings\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, early_stopping\n",
        "\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BpnsVvOu5rz"
      },
      "outputs": [],
      "source": [
        "# Subir archivo\n",
        "uploaded = files.upload()\n",
        "uploaded = files.upload()\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx-b2PfPvRH6"
      },
      "outputs": [],
      "source": [
        "# Cargar los archivos\n",
        "productos_df = pd.read_parquet(\"productos.parquet\")\n",
        "clientes_df = pd.read_parquet(\"clientes.parquet\")\n",
        "transacciones_df = pd.read_parquet(\"transacciones.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8P5zlpBv9ay"
      },
      "outputs": [],
      "source": [
        "# 1. Revisi√≥n de los datos\n",
        "print(\"Productos DataFrame:\")\n",
        "print(productos_df.head())\n",
        "\n",
        "print(\"Clientes DataFrame:\")\n",
        "print(clientes_df.head())\n",
        "\n",
        "print(\"Transacciones DataFrame:\")\n",
        "print(transacciones_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rYeNjRJukex"
      },
      "outputs": [],
      "source": [
        "print(productos_df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYAxYphv0PXl"
      },
      "outputs": [],
      "source": [
        "# Contar el n√∫mero de clientes √∫nicos en el DataFrame clientes_df\n",
        "numero_clientes = clientes_df['customer_id'].nunique()\n",
        "\n",
        "print(f\"N√∫mero total de clientes √∫nicos: {numero_clientes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoorHQkPz2yW"
      },
      "outputs": [],
      "source": [
        "# Verificar si existen customer_id comunes entre clientes y transacciones\n",
        "customer_ids_clientes = set(clientes_df['customer_id'])\n",
        "customer_ids_transacciones = set(transacciones_df['customer_id'])\n",
        "\n",
        "# Obtener los customer_id comunes\n",
        "customer_ids_comunes = customer_ids_clientes.intersection(customer_ids_transacciones)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"N√∫mero de customer_id comunes: {len(customer_ids_comunes)}\")\n",
        "print(f\"Los primeros 10 customer_id comunes: {list(customer_ids_comunes)[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCgv64gFWv2F"
      },
      "source": [
        "###An√°lisis de items, valores negativos y Target\n",
        "\n",
        "Primero, eliminamos los items negativos, esto por dos razones, primero, debido a que analizamos probabilidad de compra, como puede ser visto en cursos como IN5162 (electivo mds), lo interesante al analizar estos t√≥picos es la intenci√≥n de compra al comparar con otros productos, por ende la relevancia es en que se haya tenido la intenci√≥n de compra, por lo que aunque sean devoluciones, pueden ser por factores ex√≥genos como productos fallidos o quiebres en cadena por superaci√≥n de inventario. Por esto la soluci√≥n adeucada es no considerar items negativos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIoCI7cOWdSk"
      },
      "outputs": [],
      "source": [
        "# ELIMINACI√ìN SIMPLE DE ITEMS NEGATIVOS\n",
        "#copiamos df para mantener las estad√≠sticas originales y analizar\n",
        "transacciones_df1 = transacciones_df.copy()\n",
        "# Estad√≠sticas antes\n",
        "print(f\"Transacciones antes: {len(transacciones_df1)}\")\n",
        "print(f\"Items negativos: {len(transacciones_df1[transacciones_df1['items'] < 0])}\")\n",
        "\n",
        "# Guardar valor original\n",
        "transacciones_original = len(transacciones_df1)\n",
        "\n",
        "# Eliminar items negativos\n",
        "transacciones_df = transacciones_df1[transacciones_df1['items'] >= 0].copy()\n",
        "\n",
        "# Estad√≠sticas despu√©s\n",
        "print(f\"Transacciones despu√©s: {len(transacciones_df)}\")\n",
        "print(f\"Porcentaje eliminado: {(1 - len(transacciones_df)/transacciones_original)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lf_QwIdjb4U"
      },
      "source": [
        "Se elimina el 3%, lo que realmente podr√≠a ser un n√∫mero que no se eliminar√≠a, sin embargo al estudiarse an√°lisis conductual de compra, el an√°lisis se ve reflejado en la intenci√≥n, por ejemplo si tenemos una devoluci√≥n de 10, tambi√©n tendremos la compra de 10, donde la devoluci√≥n tendr√° un valor de -10, pero la intenci√≥n de compra est√° en la primera entrada con valor items=10, podr√≠a decirse que eliminar estos valores no hace perder informaci√≥n debido a que est√° contenido en la entrada de compra y no en la de la devoluci√≥n.\n",
        "\n",
        "Asumimos que no son valores erroneos debido a que se trata de una empresa donde su software no debiese tener un input donde se acepten valores negativos, debe ser parte del procesamiento del software que al hacer una devoluci√≥n acepta entradas negativas. De todas formas, esto debiese preguntarse a la empresa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjnVg8JkmOea"
      },
      "outputs": [],
      "source": [
        "# Convertir columnas categ√≥ricas a tipo 'category' (ejemplo con 'customer_type' y 'category')\n",
        "clientes_df['customer_type'] = clientes_df['customer_type'].astype('category')\n",
        "productos_df['category'] = productos_df['category'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN4JP9lamed1"
      },
      "outputs": [],
      "source": [
        "# Verificar datos duplicados en cada DataFrame\n",
        "duplicados_productos = productos_df.duplicated().sum()\n",
        "print(f\"N√∫mero de filas duplicadas en productos_df: {duplicados_productos}\")\n",
        "duplicados_clientes = clientes_df.duplicated().sum()\n",
        "print(f\"N√∫mero de filas duplicadas en clientes_df: {duplicados_clientes}\")\n",
        "duplicados_transacciones = transacciones_df.duplicated().sum()\n",
        "print(f\"N√∫mero de filas duplicadas en transacciones_df: {duplicados_transacciones}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqCqZAiimlhq"
      },
      "source": [
        "Notar que hay 885 filas duplicadas en transacciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQFYGj7PmkyP"
      },
      "outputs": [],
      "source": [
        "print(transacciones_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3P5qFCtmwNR"
      },
      "outputs": [],
      "source": [
        "# Eliminaci√≥n de duplicados\n",
        "transacciones_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyub_EQ4lHPw"
      },
      "source": [
        "###Definici√≥n de la variable Target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5W15xkjlKRv"
      },
      "source": [
        "Estamos definiendo la variable objetivo como un problema de clasificaci√≥n binaria donde predecimos si un cliente espec√≠fico comprar√° un producto espec√≠fico en una semana espec√≠fica. Esta aproximaci√≥n genera un producto cruzado de (cliente √ó producto √ó semana) donde el target = 1 indica que hubo una compra real de ese producto por ese cliente en esa semana, y target = 0 para todas las dem√°s combinaciones posibles donde no hubo compra. Este enfoque captura la naturaleza del problema de recomendaci√≥n: para cada cliente en cada per√≠odo, rankear todos los productos disponibles seg√∫n su probabilidad de compra, permitiendo al equipo comercial priorizar ofertas personalizadas. Se escoge un batch de an√°lisis=10 debido a que en el curso mencionado anteriormente vemos que es un buen n√∫mero debido a la naturaleza del ejercicio (tuvimos una tarea similar)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bbmfTOYjbHH"
      },
      "outputs": [],
      "source": [
        "# En un principio se preparan los datos temporales\n",
        "\n",
        "# Convertir fecha y crear semana del a√±o\n",
        "transacciones_df['purchase_date'] = pd.to_datetime(transacciones_df['purchase_date'])\n",
        "transacciones_df['year_week'] = transacciones_df['purchase_date'].dt.strftime('%Y-%U')\n",
        "\n",
        "print(f\"Transacciones v√°lidas (items > 0): {len(transacciones_df):,}\")\n",
        "\n",
        "# Se identifican las dimensiones activas\n",
        "\n",
        "# Clientes que han comprado al menos una vez\n",
        "clientes_activos = transacciones_df['customer_id'].unique()\n",
        "# Productos que han sido comprados al menos una vez\n",
        "productos_activos = transacciones_df['product_id'].unique()\n",
        "# Semanas con actividad comercial\n",
        "semanas_activas = sorted(transacciones_df['year_week'].unique())\n",
        "\n",
        "print(f\"Clientes activos: {len(clientes_activos):,}\")\n",
        "print(f\"Productos activos: {len(productos_activos):,}\")\n",
        "print(f\"Semanas activas: {len(semanas_activas):,}\")\n",
        "\n",
        "# Se crea la estructura de datos optimizada\n",
        "print(\"\\n3. Creando estructura optimizada...\")\n",
        "\n",
        "# Agrupar compras por cliente-semana para eficiencia\n",
        "compras_agrupadas = transacciones_df.groupby(['customer_id', 'year_week'])['product_id'].apply(set).reset_index()\n",
        "compras_agrupadas.columns = ['customer_id', 'year_week', 'productos_comprados']\n",
        "\n",
        "print(f\"Combinaciones cliente-semana √∫nicas: {len(compras_agrupadas):,}\")\n",
        "\n",
        "# Se crea el dataset con target binario\n",
        "print(\"\\n4. Generando producto cruzado con target...\")\n",
        "\n",
        "datasets = []\n",
        "\n",
        "# Procesar por lotes para manejar memoria\n",
        "batch_size = 1000  # Procesar 1000 cliente-semana por lote\n",
        "total_batches = (len(compras_agrupadas) - 1) // batch_size + 1\n",
        "\n",
        "for batch_num in range(total_batches):\n",
        "    start_idx = batch_num * batch_size\n",
        "    end_idx = min((batch_num + 1) * batch_size, len(compras_agrupadas))\n",
        "    batch = compras_agrupadas.iloc[start_idx:end_idx]\n",
        "\n",
        "    for _, row in batch.iterrows():\n",
        "        cliente = row['customer_id']\n",
        "        semana = row['year_week']\n",
        "        productos_comprados = row['productos_comprados']\n",
        "\n",
        "        # Para cada producto activo, crear registro con target\n",
        "        for producto in productos_activos:\n",
        "            target = 1 if producto in productos_comprados else 0\n",
        "\n",
        "            datasets.append({\n",
        "                'year_week': semana,\n",
        "                'customer_id': cliente,\n",
        "                'product_id': producto,\n",
        "                'target': target\n",
        "            })\n",
        "\n",
        "    if (batch_num + 1) % 10 == 0:  #cada 10 lotes\n",
        "        print(f\"Procesado lote {batch_num + 1}/{total_batches}\")\n",
        "\n",
        "# Se crea el dataset final\n",
        "print(\"\\n5. Creando DataFrame final...\")\n",
        "df_target = pd.DataFrame(datasets)\n",
        "print(f\"Dataset creado: {len(df_target):,} filas\")\n",
        "\n",
        "# 6. AN√ÅLISIS DE LA DISTRIBUCI√ìN\n",
        "print(\"\\n6. An√°lisis de distribuci√≥n del target:\")\n",
        "target_dist = df_target['target'].value_counts()\n",
        "print(target_dist)\n",
        "print(f\"Proporci√≥n de positivos: {target_dist[1]/len(df_target)*100:.2f}%\")\n",
        "\n",
        "# se une datos de clientes y productos\n",
        "print(\"\\n7. Enriqueciendo con datos adicionales...\")\n",
        "\n",
        "# Unir caracter√≠sticas de clientes\n",
        "df_final = pd.merge(df_target, clientes_df, on='customer_id', how='left')\n",
        "\n",
        "# Unir caracter√≠sticas de productos\n",
        "df_final = pd.merge(df_final, productos_df, on='product_id', how='left')\n",
        "\n",
        "print(f\"Dataset final: {len(df_final):,} filas, {len(df_final.columns)} columnas\")\n",
        "\n",
        "# Verificar integridad\n",
        "print(\"\\n8. Verificaci√≥n de integridad:\")\n",
        "print(f\"Clientes en dataset: {df_final['customer_id'].nunique():,}\")\n",
        "print(f\"Productos en dataset: {df_final['product_id'].nunique():,}\")\n",
        "print(f\"Semanas en dataset: {df_final['year_week'].nunique():,}\")\n",
        "print(f\"Valores nulos: {df_final.isnull().sum().sum()}\")\n",
        "\n",
        "# Guardar\n",
        "print(\"\\n9. Guardando dataset...\")\n",
        "df_final.to_parquet('dataset_con_target.parquet', index=False)\n",
        "print(\"‚úÖ Dataset guardado como 'dataset_con_target.parquet'\")\n",
        "\n",
        "# Muestra\n",
        "print(\"\\n10. Muestra del dataset final:\")\n",
        "print(df_final[['year_week', 'customer_id', 'product_id', 'target']].head(10))\n",
        "\n",
        "print(\" VARIABLE TARGET CREADA EXITOSAMENTE\")\n",
        "print(f\" Tama√±o del dataset: {len(df_final):,} filas\")\n",
        "print(f\"  Balance: {target_dist[1]:,} positivos vs {target_dist[0]:,} negativos\")\n",
        "print(f\" Proporci√≥n positiva: {target_dist[1]/len(df_target)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBo7C38Jz85a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjePjjLwv-r3"
      },
      "source": [
        "Ahora vemos que fuera de eso, no hay m√°s cosas interesantes a tratar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUrtZcvxwcJT"
      },
      "outputs": [],
      "source": [
        "# Verificar valores nulos en cada DataFrame\n",
        "print(\"Valores nulos en productos_df:\")\n",
        "print(productos_df.isnull().sum())\n",
        "\n",
        "print(\"Valores nulos en clientes_df:\")\n",
        "print(clientes_df.isnull().sum())\n",
        "\n",
        "print(\"Valores nulos en transacciones_df:\")\n",
        "print(transacciones_df.isnull().sum())\n",
        "\n",
        "print(\"Valores nulos en df_final:\")\n",
        "print(df_final.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLBEt2s6wpwj"
      },
      "source": [
        "Es importante notar que no existen valores nulos, por lo que es posible continuar trabajando sin realizar una limpieza respecto a los NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYBbXyhFLup2"
      },
      "outputs": [],
      "source": [
        "print(\"porcentaje de transacciones repetidas:\",duplicados_transacciones/transacciones_df.shape[0]*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE8nZgzsL6ke"
      },
      "source": [
        "Vemos que como representa tan s√≥lo un 0.35% se pueden eliminar estos datos aunque no sean errores de duplicados, debido a que son tan pocos datos que su eliminaci√≥n no quita informaci√≥n relevante para el estudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0sl3JIruzT7"
      },
      "source": [
        "## üìå EDA [0.5 puntos]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExbHZ6aGdkd21tYTI3cW8zYWhyYW5wdGlyb2s3MmRzeTV0dzQ1NWlueiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3k1hJubTtOAKPKx4k3/giphy.gif\" width=\"400\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ZvDWiLuzT8"
      },
      "source": [
        "En esta secci√≥n, se debe realizar un an√°lisis exploratorio de los datos para comprender su estructura, detectar posibles problemas y obtener informaci√≥n relevante para el entrenamiento de los modelos. La idea es que puedan detectar **patrones en los datos** que les permitan resolver el problema con mayor facilidad.\n",
        "\n",
        "Se deben responder preguntas a partir de lo que puedan visualizar/obtener, por ejemplo:\n",
        "\n",
        "- Clientes y productos\n",
        "\n",
        "    - ¬øCu√°ntos clientes √∫nicos hay en el dataset?\n",
        "\n",
        "    - ¬øCu√°ntos productos √∫nicos se encuentran en los datos?\n",
        "\n",
        "- Periodo y frecuencia\n",
        "\n",
        "    - ¬øDe qu√© periodo es la informaci√≥n disponible?\n",
        "\n",
        "    - ¬øCu√°l es la frecuencia de los registros (diaria, semanal, mensual, etc.)?\n",
        "\n",
        "- Calidad de los datos\n",
        "\n",
        "    - ¬øExisten valores nulos en el dataset? ¬øCu√°ntos? ¬øC√≥mo se pueden tratar?\n",
        "\n",
        "    - ¬øHay datos raros, como cantidades negativas o inconsistencias? Genere tests de validaci√≥n para identificar estos problemas.\n",
        "\n",
        "- Patrones de compra\n",
        "\n",
        "    - ¬øCu√°ntos productos compra en promedio cada cliente semana a semana?\n",
        "\n",
        "    - ¬øCu√°ntas transacciones ha realizado cada cliente?\n",
        "\n",
        "    - ¬øCu√°l es el periodo de recompra promedio de cada SKU?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9XUZyJt1CPb"
      },
      "outputs": [],
      "source": [
        "# N√∫mero de clientes √∫nicos\n",
        "clientes_unicos = clientes_df['customer_id'].nunique()\n",
        "print(f\"N√∫mero de clientes √∫nicos: {clientes_unicos}\")\n",
        "\n",
        "# N√∫mero de productos √∫nicos\n",
        "productos_unicos = productos_df['product_id'].nunique()\n",
        "print(f\"N√∫mero de productos √∫nicos: {productos_unicos}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSY7JQLu1Gih"
      },
      "outputs": [],
      "source": [
        "# Verificar el rango de fechas de las transacciones\n",
        "fecha_inicio = transacciones_df['purchase_date'].min()\n",
        "fecha_fin = transacciones_df['purchase_date'].max()\n",
        "print(f\"Periodo de las transacciones: {fecha_inicio} - {fecha_fin}\")\n",
        "\n",
        "# Determinar la frecuencia de los registros\n",
        "frecuencia_registros = transacciones_df['purchase_date'].diff().mode()[0]\n",
        "print(f\"Frecuencia de los registros: {frecuencia_registros}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb9Ir88fuzT8"
      },
      "outputs": [],
      "source": [
        "# Verificar si existen valores nulos\n",
        "valores_nulos = transacciones_df.isnull().sum()\n",
        "print(f\"Valores nulos en transacciones_df:\\n{valores_nulos}\")\n",
        "valores_nulos_clientes = clientes_df.isnull().sum()\n",
        "valores_nulos_productos = productos_df.isnull().sum()\n",
        "print(f\"Valores nulos en clientes_df:\\n{valores_nulos_clientes}\")\n",
        "print(f\"Valores nulos en productos_df:\\n{valores_nulos_productos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No hay valores nulos a tratar"
      ],
      "metadata": {
        "id": "kGQUQIJEk4Jb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3U0rsWw1nRs"
      },
      "outputs": [],
      "source": [
        "# N√∫mero promedio de productos comprados por cliente por semana\n",
        "transacciones_df['week'] = transacciones_df['purchase_date'].dt.isocalendar().week\n",
        "productos_por_cliente = transacciones_df.groupby(['customer_id', 'week'])['product_id'].count().mean()\n",
        "print(f\"Promedio de productos comprados por cliente por semana: {productos_por_cliente}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DNdMm881qmE"
      },
      "outputs": [],
      "source": [
        "# N√∫mero de transacciones por cliente\n",
        "transacciones_por_cliente = transacciones_df.groupby('customer_id')['order_id'].nunique()\n",
        "print(f\"Promedio de transacciones por cliente: {transacciones_por_cliente.mean()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nujjTZs1xXL"
      },
      "outputs": [],
      "source": [
        "# Periodo de recompra promedio por SKU\n",
        "# Calcular el tiempo entre compras de un mismo producto\n",
        "transacciones_df['purchase_date'] = pd.to_datetime(transacciones_df['purchase_date'])\n",
        "transacciones_df['next_purchase'] = transacciones_df.groupby('product_id')['purchase_date'].shift(-1)\n",
        "transacciones_df['days_between'] = (transacciones_df['next_purchase'] - transacciones_df['purchase_date']).dt.days\n",
        "recompra_promedio_sku = transacciones_df.groupby('product_id')['days_between'].mean()\n",
        "print(f\"Periodo de recompra promedio por SKU:\\n{recompra_promedio_sku.describe()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHvHumINNTAS"
      },
      "source": [
        "Vemos que el promedio de recompra es negativo, esto nos muestra que los datos no est√°n ordenados por fecha de compra, por ende los ordenaremos por fecha de compra para tener informaci√≥n √∫til\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYQpDo1BNc4h"
      },
      "outputs": [],
      "source": [
        "# 1. Ordenar el DataFrame por product_id y purchase_date\n",
        "transacciones_df.sort_values(by=['product_id', 'purchase_date'], inplace=True)\n",
        "\n",
        "# 2. Calcular la fecha de la siguiente compra usando shift(-1)\n",
        "transacciones_df['next_purchase'] = transacciones_df.groupby('product_id')['purchase_date'].shift(-1)\n",
        "\n",
        "# 3. Calcular los d√≠as entre compras\n",
        "transacciones_df.dropna(subset=['next_purchase'], inplace=True)\n",
        "transacciones_df['days_between'] = (transacciones_df['next_purchase'] - transacciones_df['purchase_date']).dt.days\n",
        "\n",
        "# 4. Calcular el per√≠odo de recompra promedio por SKU\n",
        "recompra_promedio_sku = transacciones_df.groupby('product_id')['days_between'].mean()\n",
        "\n",
        "# 5. Mostrar los resultados\n",
        "print(\"Periodo de recompra promedio por SKU:\\n\", recompra_promedio_sku.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIXLrWNiWaAR"
      },
      "source": [
        "Con el orden, podemos ver que tenemos valores que hacen sentido. El promedio entre recompra es de 8 d√≠as. Pero tambi√©n hay productos que se venden menos o m√°s, para ello veamos como distribuye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cn_uSeAXmeU"
      },
      "outputs": [],
      "source": [
        "# Define los intervalos (bins) de 10 d√≠as. El √∫ltimo valor es el m√°ximo m√°s 10 para incluir todo.\n",
        "bins = range(0, int(transacciones_df['days_between'].max()) + 11, 10)\n",
        "\n",
        "# Crea etiquetas para cada intervalo, por ejemplo: '0-9 d√≠as', '10-19 d√≠as', etc.\n",
        "labels = [f'{i}-{i+9} d√≠as' for i in range(0, int(transacciones_df['days_between'].max()) + 1, 10)]\n",
        "\n",
        "# Usa la funci√≥n pd.cut() para categorizar los datos\n",
        "intervalos_dias = pd.cut(transacciones_df['days_between'], bins=bins, right=False, labels=labels)\n",
        "\n",
        "# Cuenta la frecuencia de cada intervalo\n",
        "frecuencia_dias = intervalos_dias.value_counts().sort_index()\n",
        "\n",
        "# Convierte la serie en un DataFrame para un formato de tabla m√°s claro\n",
        "tabla_frecuencias = frecuencia_dias.reset_index()\n",
        "tabla_frecuencias.columns = ['Intervalo (D√≠as)', 'Frecuencia']\n",
        "\n",
        "# Imprime la tabla\n",
        "print(tabla_frecuencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucsI1pGgYcmA"
      },
      "source": [
        "Entre los primeros 10 d√≠as se concentra la mayor√≠a de ventas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GICKLqjYgfO"
      },
      "outputs": [],
      "source": [
        "# Define los intervalos (bins) de 1 d√≠a, solo para los primeros 10 d√≠as\n",
        "bins = range(0, 11, 1)\n",
        "\n",
        "# Crea las etiquetas para cada intervalo, por ejemplo: '0-1 d√≠as', '1-2 d√≠as', etc.\n",
        "labels = [f'{i}-{i+1} d√≠as' for i in range(0, 10)]\n",
        "\n",
        "# Filtra el DataFrame para incluir solo los datos de los primeros 10 d√≠as\n",
        "transacciones_primeros_10_dias = transacciones_df[transacciones_df['days_between'] <= 10]\n",
        "\n",
        "# Usa la funci√≥n pd.cut() para categorizar los datos filtrados en los intervalos definidos\n",
        "intervalos_dias = pd.cut(transacciones_primeros_10_dias['days_between'], bins=bins, right=False, labels=labels)\n",
        "\n",
        "# Cuenta la frecuencia de cada intervalo\n",
        "frecuencia_dias = intervalos_dias.value_counts().sort_index()\n",
        "\n",
        "# Convierte la serie en un DataFrame para un formato de tabla m√°s claro\n",
        "tabla_frecuencias = frecuencia_dias.reset_index()\n",
        "tabla_frecuencias.columns = ['Intervalo (D√≠as)', 'Frecuencia']\n",
        "\n",
        "# Imprime la tabla\n",
        "print(tabla_frecuencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IKbF013YpCV"
      },
      "source": [
        "Desmenusando a√∫n m√°s, es posible ver que la mayor√≠a se venden entre 0-1 d√≠as, informaci√≥n que puede ser relevante para el an√°lisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16KNju2n5Ao"
      },
      "source": [
        "Veamos como se comporta la variable target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Nw7cD2n2yV"
      },
      "outputs": [],
      "source": [
        "# Distribuci√≥n temporal del target\n",
        "print(\"Distribuci√≥n del target por semana:\")\n",
        "target_por_semana = df_final.groupby('year_week')['target'].mean()\n",
        "print(target_por_semana)\n",
        "\n",
        "# An√°lisis de clientes m√°s activos\n",
        "clientes_activos = df_final.groupby('customer_id')['target'].sum().sort_values(ascending=False)\n",
        "print(f\"Top 10 clientes con m√°s compras: {clientes_activos.head(10)}\")\n",
        "\n",
        "# Productos m√°s populares\n",
        "productos_populares = df_final.groupby('product_id')['target'].sum().sort_values(ascending=False)\n",
        "print(f\"Top 10 productos m√°s comprados: {productos_populares.head(10)}\")\n",
        "\n",
        "# Relaci√≥n entre tipo de cliente y compras\n",
        "tipo_cliente_target = df_final.groupby('customer_type')['target'].mean()\n",
        "print(f\"Tasa de compra por tipo de cliente: {tipo_cliente_target}\")\n",
        "\n",
        "# Segmentaci√≥n de productos vs target\n",
        "segmento_target = df_final.groupby('segment')['target'].mean()\n",
        "print(f\"Tasa de compra por segmento: {segmento_target}\")\n",
        "\n",
        "# Visualizaciones clave\n",
        "# Distribuci√≥n del target por categor√≠as importantes\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Target por tipo de cliente\n",
        "df_final.groupby('customer_type')['target'].mean().plot(kind='bar', ax=axes[0,0], title='Tasa de Compra por Tipo de Cliente')\n",
        "\n",
        "# Target por segmento de producto\n",
        "df_final.groupby('segment')['target'].mean().plot(kind='bar', ax=axes[0,1], title='Tasa de Compra por Segmento')\n",
        "\n",
        "# Target por categor√≠a\n",
        "df_final.groupby('category')['target'].mean().plot(kind='bar', ax=axes[1,0], title='Tasa de Compra por Categor√≠a')\n",
        "\n",
        "# Evoluci√≥n temporal\n",
        "target_por_semana.plot(ax=axes[1,1], title='Evoluci√≥n Temporal de la Tasa de Compra')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq_A7U0wYEKc"
      },
      "source": [
        "El eda espec√≠fico de la v targer muestra patrones claros en el comportamiento de compra seg√∫n tipo de cliente, segmento y evoluci√≥n temporal. En cuanto al tipo de cliente, los minimarkets presentan la tasa de compra m√°s alta (11.5%), seguidos de mayoristas y tiendas de conveniencia, lo que sugiere que estos formatos realizan compras m√°s seguido, por ende tienen una propensi√≥n a concretarlas. En cambio, abarrotes y canal fr√≠o muestran tasas bajas, indicando una menor frecuencia de compra.\n",
        "\n",
        "Por segmento, los clientes low y medium concentran las mayores tasas (5.9% y 5.3%, respectivamente), mientras que los premium y high registran menor participaci√≥n, posiblemente debido a patrones de compra m√°s selectivos o un menor volumen transaccional debido a la categor√≠a premium. En la evoluci√≥n temporal, se observa una estabilidad moderada en la tasa de compra durante el a√±o, con un leve repunte hacia las semanas finales (semana 50 y 51), alcanzando un m√°ximo de 5%, lo que podr√≠a asociarse a la estacionalidad de fin de a√±o.\n",
        "\n",
        "Los top 10 clientes evidencian una concentraci√≥n significativa, con el cliente principal acumulando 996 compras, lo que refleja un comportamiento donde pocos clientes explican una parte relevante de las ventas. De todas forma, los productos m√°s comprados muestran alta concentraci√≥n en algunos √≠tems, destacando el producto 1370 con m√°s de 28.000 compras. As√≠, el EDA sugiere que la demanda se concentra tanto en ciertos tipos de clientes como en productos espec√≠ficos, mientras que la tasa de compra var√≠a seg√∫n el formato comercial y la estacionalidad temporal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Gz7wkuuzT8"
      },
      "source": [
        "## üìå Holdout [0.25 puntos]\n",
        "\n",
        "Para evaluar correctamente el modelo y garantizar su capacidad de generalizaci√≥n, se deben dividir los datos en tres conjuntos:\n",
        "- `Entrenamiento` : Para ajustar los par√°metros.\n",
        "- `Validaci√≥n`: Para optimizar hiperpar√°metros y seleccionar el mejor modelo.\n",
        "- `Prueba` : Para evaluar el rendimiento final en datos no vistos.\n",
        "\n",
        "üëÄ **Hint**: *Recuerde que los datos tienen una temporalidad que debe considerarse al momento de separarlos, para evitar fugas de informaci√≥n. Es importante justificar la estrategia de partici√≥n elegida y visualizar la distribuci√≥n temporal de los conjuntos generados*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Tdu-bXrtFg"
      },
      "source": [
        "Lo hacemos con df_final para evitar data leakege"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQQ5CNPO3lqc"
      },
      "outputs": [],
      "source": [
        "# Ordenar por year_week\n",
        "df_final_sorted = df_final.sort_values(by='year_week').reset_index(drop=True)\n",
        "\n",
        "# Partici√≥n temporal 80-10-10\n",
        "train_size = int(0.8 * len(df_final_sorted))\n",
        "val_size = int(0.1 * len(df_final_sorted))\n",
        "\n",
        "train_df = df_final_sorted.iloc[:train_size].copy()\n",
        "val_df = df_final_sorted.iloc[train_size:train_size + val_size].copy()\n",
        "test_df = df_final_sorted.iloc[train_size + val_size:].copy()\n",
        "\n",
        "# Agregar etiquetas\n",
        "train_df['conjunto'] = 'Entrenamiento'\n",
        "val_df['conjunto'] = 'Validaci√≥n'\n",
        "test_df['conjunto'] = 'Prueba'\n",
        "\n",
        "print(f\"Train: {len(train_df)} registros ({len(train_df)/len(df_final_sorted)*100:.1f}%)\")\n",
        "print(f\"Val: {len(val_df)} registros ({len(val_df)/len(df_final_sorted)*100:.1f}%)\")\n",
        "print(f\"Test: {len(test_df)} registros ({len(test_df)/len(df_final_sorted)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribuci√≥n del target\n",
        "print(\"\\nüìä Distribuci√≥n del Target:\")\n",
        "print(\"Train:\\n\", train_df['target'].value_counts(normalize=True))\n",
        "print(\"\\nVal:\\n\", val_df['target'].value_counts(normalize=True))\n",
        "print(\"\\nTest:\\n\", test_df['target'].value_counts(normalize=True))\n",
        "\n",
        "# Visualizaci√≥n por year_week\n",
        "df_concatenado = pd.concat([train_df, val_df, test_df])\n",
        "\n",
        "df_semanal = df_concatenado.groupby(['year_week', 'conjunto']).size().reset_index(name='count')\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(\n",
        "    data=df_semanal,\n",
        "    x='year_week',\n",
        "    y='count',\n",
        "    hue='conjunto',\n",
        "    dodge=False,\n",
        "    palette={'Entrenamiento': 'green', 'Validaci√≥n': 'orange', 'Prueba': 'red'}\n",
        ")\n",
        "\n",
        "plt.title('Distribuci√≥n de Registros por Semana y Conjunto', fontsize=18)\n",
        "plt.xlabel('A√±o-Semana', fontsize=14)\n",
        "plt.ylabel('Cantidad de Registros', fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Conjunto')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta forma se deja en evidencia que se separa el dataset tal como se hab√≠a soliictado en las instrucciones."
      ],
      "metadata": {
        "id": "7S5iR6EebuOO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3B8y2w5uzT9"
      },
      "source": [
        "## üìå Feature Engineering [0.5 puntos]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/CmXZSSC.gif\" width=\"300\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcKcm1gsuzT9"
      },
      "source": [
        "En esta secci√≥n, se deben construir pipelines para automatizar el preprocesamiento de los datos, lo cual garantizar√° que el flujo de trabajo sea reproducible y eficiente para esta entrega y las futuras. El objetivo es aplicar una serie de transformaciones en un orden definido para asegurar que los datos est√©n listos para los modelos a entrenar. El pipeline final debe incluir las t√©cnicas de pre-procesamiento que se deben aplicar a los distintos datos (seg√∫n lo que consideren necesario para el problema). Por ejemplo:\n",
        "\n",
        "- **Imputaci√≥n de valores nulos**: Manejo de datos faltantes mediante estrategias adecuadas (media, mediana, moda, interpolaci√≥n, etc.).\n",
        "\n",
        "- **Transformaciones personalizadas**: Uso de ColumnTransformer para aplicar diferentes transformaciones a columnas espec√≠ficas.\n",
        "\n",
        "- **Codificaci√≥n de variables categ√≥ricas**: Convertir datos categ√≥ricos a un formato num√©rico adecuado (One-Hot Encoding, Label Encoding, etc.).\n",
        "\n",
        "- **Discretizaci√≥n de variables**: Conversi√≥n de variables num√©ricas continuas en categor√≠as si son relevantes para el desempe√±o del modelo a entrenar.\n",
        "\n",
        "- **Estandarizaci√≥n o normalizaci√≥n** : Ajustar la escala de los datos para mejorar el rendimiento de los algoritmos sensibles a la magnitud de las variables.\n",
        "\n",
        "- **Eliminaci√≥n o transformaci√≥n de valores at√≠picos**: Identificar y tratar con datos outliers para mejorar la robustez del modelo.\n",
        "\n",
        "- **Nuevas caracter√≠sticas** : Creaci√≥n de variables adicionales que puedan aportar informaci√≥n relevante al modelo.\n",
        "\n",
        "Cada una de estas transformaciones debe ser justificada en funci√≥n de su relevancia para el problema y los datos, y es importante evaluar su impacto en el rendimiento del modelo. Adem√°s, el pipeline debe ser flexible y modular para poder probar diferentes configuraciones de preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGG7nsgDuzT9"
      },
      "outputs": [],
      "source": [
        "## Transformadores\n",
        "\n",
        "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Trata valores at√≠picos usando capping basado en percentiles.\n",
        "    M√°s robusto que eliminar outliers, mantiene todos los datos.\n",
        "    \"\"\"\n",
        "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
        "        self.lower_percentile = lower_percentile\n",
        "        self.upper_percentile = upper_percentile\n",
        "        self.lower_cap = None\n",
        "        self.upper_cap = None\n",
        "        self.n_features_in_ = None # Initialize n_features_in_\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.lower_cap = np.percentile(X, self.lower_percentile, axis=0)\n",
        "        self.upper_cap = np.percentile(X, self.upper_percentile, axis=0)\n",
        "        self.n_features_in_ = X.shape[1] # Set n_features_in_\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_capped = X.copy()\n",
        "        for i in range(X.shape[1]):\n",
        "            X_capped[:, i] = np.clip(X_capped[:, i], self.lower_cap[i], self.upper_cap[i])\n",
        "        return X_capped\n",
        "\n",
        "    # Add get_feature_names_out method\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        if input_features is None:\n",
        "            if self.n_features_in_ is None:\n",
        "                raise RuntimeError(\"This transformer has not been fitted yet.\")\n",
        "            # Default feature names if input_features is None\n",
        "            return np.asarray([f\"x{i}\" for i in range(self.n_features_in_)], dtype=object)\n",
        "        else:\n",
        "            return np.asarray(input_features, dtype=object)\n",
        "\n",
        "\n",
        "\n",
        "class LogTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Aplica transformaci√≥n logar√≠tmica a variables con distribuci√≥n sesgada.\n",
        "    √ötil para normalizar distribuciones y reducir el efecto de outliers.\n",
        "    \"\"\"\n",
        "    def __init__(self, offset=1):\n",
        "        self.offset = offset  # Para evitar log(0)\n",
        "        self.n_features_in_ = None # Initialize n_features_in_\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.n_features_in_ = X.shape[1] # Set n_features_in_\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.log(X + self.offset)\n",
        "\n",
        "    # Add get_feature_names_out method\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        if input_features is None:\n",
        "            if self.n_features_in_ is None:\n",
        "                raise RuntimeError(\"This transformer has not been fitted yet.\")\n",
        "            # Default feature names if input_features is None\n",
        "            return np.asarray([f\"x{i}\" for i in range(self.n_features_in_)], dtype=object)\n",
        "        else:\n",
        "            return np.asarray(input_features, dtype=object)\n",
        "\n",
        "\n",
        "## Identificacion de columnas\n",
        "\n",
        "# Variables categ√≥ricas\n",
        "categorical_cols = [\n",
        "    'region_id',\n",
        "    'zone_id',\n",
        "    'customer_type',\n",
        "    'brand',\n",
        "    'category',\n",
        "    'sub_category',\n",
        "    'segment',\n",
        "    'package'\n",
        "]\n",
        "\n",
        "# Variables num√©ricas continuas est√°ndar\n",
        "numeric_standard_cols = [\n",
        "    'Y',\n",
        "    'X',\n",
        "    'size'\n",
        "]\n",
        "\n",
        "# Variables num√©ricas que se benefician de transformaci√≥n logar√≠tmica\n",
        "# (distribuciones sesgadas con valores altos)\n",
        "numeric_log_cols = [\n",
        "    'num_deliver_per_week',\n",
        "    'num_visit_per_week'\n",
        "]\n",
        "\n",
        "\n",
        "## Justificacion\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "JUSTIFICACI√ìN DE CADA PASO:\n",
        "\n",
        "1. IMPUTACI√ìN (SimpleImputer):\n",
        "   - Mediana para num√©ricas: Robusta ante outliers\n",
        "   - Moda para categ√≥ricas: Mantiene la distribuci√≥n original\n",
        "   - Cr√≠tico: df_final no tiene nulos, pero el pipeline debe ser robusto\n",
        "\n",
        "2. TRATAMIENTO DE OUTLIERS (OutlierCapper):\n",
        "   - Percentiles 1-99: Mantiene 98% de datos, reduce extremos\n",
        "   - Alternativa a eliminaci√≥n: No perdemos informaci√≥n\n",
        "   - Importante para: num_deliver_per_week, num_visit_per_week, size\n",
        "\n",
        "3. TRANSFORMACI√ìN LOGAR√çTMICA (LogTransformer):\n",
        "   - Para variables con distribuci√≥n sesgada\n",
        "   - Reduce impacto de valores extremos\n",
        "   - Mejora rendimiento en modelos lineales\n",
        "\n",
        "4. ESCALADO (RobustScaler):\n",
        "   - RobustScaler vs StandardScaler: Usa mediana y IQR (m√°s robusto)\n",
        "   - Cr√≠tico para: modelos sensibles a escala (SVM, KNN, Regresi√≥n)\n",
        "   - No necesario para: √°rboles de decisi√≥n, random forest\n",
        "\n",
        "5. CODIFICACI√ìN CATEG√ìRICA (OneHotEncoder):\n",
        "   - One-Hot: Para variables nominales sin orden (brand, category, etc.)\n",
        "   - handle_unknown='ignore': Robustez ante nuevas categor√≠as en test\n",
        "   - Alternativa: Target Encoding (si hay muchas categor√≠as √∫nicas)\n",
        "\n",
        "6. COLUMNAS PASSTHROUGH:\n",
        "   - customer_id, product_id: Identificadores (√∫tiles para an√°lisis post-modelo)\n",
        "   - year_week: Feature temporal importante\n",
        "   - target: NUNCA debe ser transformada en el pipeline\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Pipelines por tipo de variable\n",
        "\n",
        "# Pipeline para variables num√©ricas est√°ndar\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('outlier_capper', OutlierCapper(lower_percentile=1, upper_percentile=99)),\n",
        "    ('scaler', RobustScaler())  # M√°s robusto que StandardScaler\n",
        "])\n",
        "\n",
        "# Pipeline para variables que necesitan transformaci√≥n logar√≠tmica\n",
        "log_numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('log_transform', LogTransformer(offset=1)),\n",
        "    ('outlier_capper', OutlierCapper(lower_percentile=1, upper_percentile=99)),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para variables categ√≥ricas\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(\n",
        "        drop='first',           # Evita multicolinealidad\n",
        "        handle_unknown='ignore', # Maneja categor√≠as nuevas\n",
        "        sparse_output=False      # Retorna array denso\n",
        "    ))\n",
        "])\n",
        "\n",
        "## Colum transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num_standard', numeric_pipeline, numeric_standard_cols),\n",
        "        ('num_log', log_numeric_pipeline, numeric_log_cols),\n",
        "        ('cat', categorical_pipeline, categorical_cols)\n",
        "    ],\n",
        "    remainder='drop',  # Elimina year_week, customer_id, product_id, target\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# Pipeline final completo\n",
        "feature_engineering_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "\n",
        "## Aplicaci√≥n del pipeline\n",
        "\n",
        "# Separar features y target\n",
        "X_train = train_df.drop(columns=['target'])\n",
        "y_train = train_df['target']\n",
        "\n",
        "X_val = val_df.drop(columns=['target'])\n",
        "y_val = val_df['target']\n",
        "\n",
        "X_test = test_df.drop(columns=['target'])\n",
        "y_test = test_df['target']\n",
        "\n",
        "# Ajustar el pipeline solo con datos de entrenamiento\n",
        "feature_engineering_pipeline.fit(X_train)\n",
        "\n",
        "# Transformar todos los conjuntos\n",
        "X_train_transformed = feature_engineering_pipeline.transform(X_train)\n",
        "X_val_transformed = feature_engineering_pipeline.transform(X_val)\n",
        "X_test_transformed = feature_engineering_pipeline.transform(X_test)\n",
        "\n",
        "print(\" Pipeline aplicado exitosamente!\")\n",
        "print(f\"Shape original: {X_train.shape}\")\n",
        "print(f\"Shape transformado: {X_train_transformed.shape}\")\n",
        "print(f\"\\nCantidad de features despu√©s de One-Hot Encoding: {X_train_transformed.shape[1]}\")\n",
        "\n",
        "# Obtener nombres de features transformadas\n",
        "\n",
        "feature_names = feature_engineering_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "print(f\"\\n Primeras 20 features transformadas:\")\n",
        "print(feature_names[:20])\n",
        "\n",
        "# Convertir a DataFrame para mejor visualizaci√≥n (opcional)\n",
        "X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
        "X_val_df = pd.DataFrame(X_val_transformed, columns=feature_names)\n",
        "X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
        "\n",
        "print(\"\\n DataFrames transformados creados\")\n",
        "print(X_train_df.head())\n",
        "\n",
        "# Verificaci√≥n de calidad\n",
        "\n",
        "print(\"\\n Verificaci√≥n de calidad del pipeline:\")\n",
        "print(f\"- Valores nulos en train: {pd.DataFrame(X_train_transformed).isnull().sum().sum()}\")\n",
        "print(f\"- Valores nulos en val: {pd.DataFrame(X_val_transformed).isnull().sum().sum()}\")\n",
        "print(f\"- Valores nulos en test: {pd.DataFrame(X_test_transformed).isnull().sum().sum()}\")\n",
        "print(f\"- Valores infinitos en train: {np.isinf(X_train_transformed).sum()}\")\n",
        "\n",
        "# Guardar el pipeline para uso futuro\n",
        "joblib.dump(feature_engineering_pipeline, 'feature_engineering_pipeline.pkl')\n",
        "print(\"\\n Pipeline guardado como 'feature_engineering_pipeline.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nPnvygSuzT9"
      },
      "source": [
        "## üìå Baseline [0.25 puntos]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExN3lzeGFqZmU3NzJrZHllNjRmaHVzczJpZ29rdHdlMzVpZnQwNXo1diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/qAtZM2gvjWhPjmclZE/giphy.gif\" width=\"300\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egx3W47CuzT-"
      },
      "source": [
        "En esta secci√≥n se debe construir el modelo m√°s sencillo posible que pueda resolver el problema planteado, conocido como **Modelo baseline**. Su prop√≥sito es servir como referencia para comparar el rendimiento de los modelos m√°s avanzados desarrollados en etapas posteriores.  \n",
        "\n",
        "Pasos requeridos:  \n",
        "- Implemente, entrene y eval√∫e un modelo b√°sico utilizando un pipeline.  \n",
        "- Aseg√∫rese de incluir en el pipeline las transformaciones del preprocesamiento realizadas previamente junto con un clasificador b√°sico.  \n",
        "- Eval√∫e el modelo y presente el informe de m√©tricas utilizando **`classification_report`**.  \n",
        "\n",
        "Documente claramente c√≥mo se cre√≥ el modelo, las decisiones tomadas y los resultados obtenidos. Este modelo ser√° la base comparativa en las secciones posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEQ_YD8it36s"
      },
      "outputs": [],
      "source": [
        "# 1. Define the Baseline Model (DummyClassifier)\n",
        "baseline_model = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
        "\n",
        "baseline_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', feature_engineering_pipeline.named_steps['preprocessor']),\n",
        "    ('classifier', baseline_model)\n",
        "])\n",
        "\n",
        "\n",
        "# 2. Separar x e y\n",
        "X_train = train_df.drop(columns=['target', 'conjunto'])\n",
        "y_train = train_df['target']\n",
        "\n",
        "X_val = val_df.drop(columns=['target', 'conjunto'])\n",
        "y_val = val_df['target']\n",
        "\n",
        "X_test = test_df.drop(columns=['target', 'conjunto'])\n",
        "y_test = test_df['target']\n",
        "\n",
        "\n",
        "# 4. entrenar el modelo\n",
        "\n",
        "baseline_pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 5. Evaluar\n",
        "\n",
        "print(\"--- Baseline Model Evaluation ---\")\n",
        "\n",
        "# evaluar en training set\n",
        "y_pred_train = baseline_pipeline.predict(X_train)\n",
        "print(\"\\nClassification Report (Training Set):\")\n",
        "\n",
        "train_report = classification_report(y_train, y_pred_train)\n",
        "print(train_report)\n",
        "cm_train = confusion_matrix(y_train, y_pred_train)\n",
        "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=baseline_pipeline.classes_)\n",
        "disp_train.plot()\n",
        "plt.title('Confusion Matrix (Training Set)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluar el val set\n",
        "y_pred_val = baseline_pipeline.predict(X_val)\n",
        "print(\"\\nClassification Report (Validation Set):\")\n",
        "\n",
        "val_report = classification_report(y_val, y_pred_val)\n",
        "print(val_report)\n",
        "cm_val = confusion_matrix(y_val, y_pred_val)\n",
        "disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=baseline_pipeline.classes_)\n",
        "disp_val.plot()\n",
        "plt.title('Confusion Matrix (Validation Set)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# para train set evaluar\n",
        "y_pred_test = baseline_pipeline.predict(X_test)\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "\n",
        "test_report = classification_report(y_test, y_pred_test)\n",
        "print(test_report)\n",
        "cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=baseline_pipeline.classes_)\n",
        "disp_test.plot()\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Baseline Model Trained and Evaluated.\")\n",
        "print(\"Using DummyClassifier with 'most_frequent' strategy.\")\n",
        "print(\"This baseline predicts the majority class (which is 0 - no purchase).\")\n",
        "print(\"It serves as a basic reference point for model performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eucaBydrxX6w"
      },
      "source": [
        "Ac√° podemos ver que el modelo es muy simple, de hecho luego de indagar en el modelo, es posible ver que la raz√≥n es que este es un modelo extremadamente sencillo que analiza y entiende la clase mayoritaria y tira todo a esa clase, por eso en la clase 0 tenemos m√©tricas tan buenas pero en la clase 1 son m√©tricas horribles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvZ5S1muzT_"
      },
      "source": [
        "## üìå Elecci√≥n de modelo [0.75 puntos]\n",
        "\n",
        "En esta secci√≥n deben escoger un modelo que se adapte a las necesidades del negocio. Para esto, pruebe al menos 3 modelos y desarrolle los siguientes aspectos para cada uno:\n",
        "\n",
        "- **Estructura y diferencias entre los modelos**: Explicar brevemente cada uno y sus hip√©rpar√°metros de mayor importancia.\n",
        "- **Clasificadores recomendados**:\n",
        "  - `LogisticRegression`\n",
        "  - `KNeighborsClassifier`\n",
        "  - `DecisionTreeClassifier`\n",
        "  - `SVC`\n",
        "  - `RandomForestClassifier`\n",
        "  - `LightGBMClassifier` (del paquete `lightgbm`)\n",
        "  - `XGBClassifier` (del paquete `xgboost`)\n",
        "  - Otro (seg√∫n lo que se estime adecuado)\n",
        "  \n",
        "- **Evaluaci√≥n de resultados**: Se utilizar√° el **`classification_report`** para evaluar el rendimiento de cada modelo, destacando m√©tricas clave como precisi√≥n, recall y F1-score. **Importante: No optimicen hiperpar√°metros, la idea es hacer una selecci√≥n r√°pida del modelo.**\n",
        "\n",
        "**Nota:** Pueden ocupar mas de 1 **instancia** de modelo para resolver el problema (e.g: (modelo_1, grupo_1), (modelo_2, grupo_2), ...).\n",
        "  \n",
        "A continuaci√≥n, se deben responder las siguientes preguntas para evaluar el rendimiento de los modelos entrenados:\n",
        "\n",
        "1. ¬øHay alg√∫n clasificador que supere al modelo baseline?  \n",
        "2. ¬øCu√°l es el mejor clasificador entrenado y por qu√©?  \n",
        "3. ¬øQu√© factores hacen que el mejor clasificador sea superior a los otros?  \n",
        "4. En t√©rminos de `tiempo de entrenamiento`, ¬øQu√© modelo considera m√°s adecuado para experimentar con grillas de optimizaci√≥n?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JS5b3adNZGm"
      },
      "outputs": [],
      "source": [
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.metrics\")\n",
        "\n",
        "# === usamos preprocesado de antes ===\n",
        "\n",
        "\n",
        "# === balanceo ===\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "print(\"Calculated Class Weights:\", class_weight_dict)\n",
        "\n",
        "# === Define models ===\n",
        "models = {\n",
        "    \"LightGBM\": LGBMClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=100,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        random_state=42,\n",
        "        solver='liblinear',\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# === Separar x e y===\n",
        "X_train = train_df.drop(columns=['target', 'conjunto'])\n",
        "y_train = train_df['target']\n",
        "\n",
        "X_val = val_df.drop(columns=['target', 'conjunto'])\n",
        "y_val = val_df['target']\n",
        "\n",
        "X_test = test_df.drop(columns=['target', 'conjunto'])\n",
        "y_test = test_df['target']\n",
        "\n",
        "## === entrenar y evaluar modelos ===\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n--- Training and evaluating {model_name} ---\")\n",
        "\n",
        "    # pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', feature_engineering_pipeline.named_steps['preprocessor']),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # entrenar modelo\n",
        "    if model_name == \"LightGBM\":\n",
        "        #LightGBM\n",
        "        pipeline.fit(\n",
        "            X_train, y_train,\n",
        "            classifier__eval_set=[(pipeline.named_steps['preprocessor'].transform(X_val), y_val)],\n",
        "            classifier__callbacks=[early_stopping(stopping_rounds=10)]\n",
        "        )\n",
        "    else:\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # === Evaluar modelo ===\n",
        "    print(f\"\\nEvaluation for {model_name}:\")\n",
        "\n",
        "    # entrenar\n",
        "    y_pred_train = pipeline.predict(X_train)\n",
        "    print(\"\\nClassification Report (Training Set):\")\n",
        "    print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "    # Validar\n",
        "    y_pred_val = pipeline.predict(X_val)\n",
        "    print(\"\\nClassification Report (Validation Set):\")\n",
        "    print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "    # Testeo\n",
        "    y_pred_test = pipeline.predict(X_test)\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n Model selection evaluation complete. Review the classification reports above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VocMMQe3PufI"
      },
      "source": [
        "S√≠, los clasificadores entrenados superan al modelo baseline. Mientras el baseline ten√≠a un recall de 0.00 para la clase minoritaria y un f1-score nulo, los modelos entrenados lograron detectar correctamente una proporci√≥n significativa de casos positivos, mejorando notablemente el desempe√±o en la clase minoritaria sin sacrificar demasiado la precisi√≥n de la clase mayoritaria. En otras palabras, ahora tenemos modelos que no arrojan al azar\n",
        "\n",
        "\n",
        "El mejor clasificador entrenado es el Decision Tree. Su desempe√±o en el conjunto de test muestra un f1-score macro promedio m√°s alto (0.60) y un recall elevado para la clase positiva (0.86), lo que indica que captura mejor los verdaderos positivos que LightGBM y Logistic Regression, equilibrando precisi√≥n y sensibilidad de manera m√°s efectiva.\n",
        "\n",
        "La superioridad del Decision Tree se debe a su capacidad para capturar relaciones no lineales y efectos de interacci√≥n entre variables, algo que modelos lineales como la regresi√≥n log√≠stica no pueden. Adem√°s, su estructura jer√°rquica le permite enfocarse en patrones espec√≠ficos de la clase minoritaria, y los pesos de clase ayudan a mitigar el desbalance, aumentando su capacidad de generalizaci√≥n en datos complejos.\n",
        "\n",
        "En t√©rminos de tiempo de entrenamiento, la regresi√≥n log√≠stica es el modelo m√°s adecuado para experimentar con grillas de optimizaci√≥n. Es r√°pida de entrenar y permite explorar m√∫ltiples combinaciones de hiperpar√°metros sin un costo computacional elevado, a diferencia de LightGBM o Decision Tree, que requieren m√°s tiempo para entrenar y ajustar con muchas configuraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8QWM2DSuzT_"
      },
      "source": [
        "## üìå Optimizaci√≥n de Hiperpar√°metros [1.0 puntos]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExcXJkNzdhYjlneHplaGpsbnVkdzh5dnY3Y2VyaTIzamszdGR1czJ2diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/2rqEdFfkMzXmo/giphy.gif\" width=\"300\" height=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkE0-twPuzT_"
      },
      "source": [
        "A partir de su an√°lisis anterior, se debe proceder a optimizar el rendimiento del modelo seleccionado mediante la optimizaci√≥n de sus hiperpar√°metros. Para ello, se espera que implementen `Optuna` para optimizar no solo los hiperpar√°metros del modelo, sino tambi√©n los de los preprocesadores utilizados (por ejemplo, OneHot Encoding, Scalers, etc.).\n",
        "\n",
        "Al desarrollar este proceso, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
        "\n",
        "- ¬øQu√© m√©trica decidieron optimizar y por qu√©?\n",
        "\n",
        "- ¬øQu√© hiperpar√°metro tuvo un mayor impacto en el rendimiento de su modelo?\n",
        "\n",
        "- ¬øCu√°nto mejor√≥ el rendimiento del modelo despu√©s de la optimizaci√≥n de hiperpar√°metros?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ltRUNhoQMaa"
      },
      "source": [
        "Usaremos regresi√≥n log√≠stica ya que no hay tanta diferencia entre los modelos, pero si es clave que se demora menos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki4EKMZFQk-D"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS CON OPTUNA ===\n",
        "# Versi√≥n optimizada para Google Colab (bajo consumo de RAM)\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc  # Para liberaci√≥n de memoria\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS CON OPTUNA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# === VERIFICACI√ìN DE VARIABLES ===\n",
        "\n",
        "\n",
        "# LO DE AC√Å ABAJO SE DEBE ELIMINAR Y DEJAR SOLO 20000 DATOS\n",
        "\n",
        "try:\n",
        "    print(f\"train_df shape: {train_df.shape}\")\n",
        "    print(f\"val_df shape: {val_df.shape}\")\n",
        "    print(f\"test_df shape: {test_df.shape}\")\n",
        "\n",
        "    # Usar muestra m√°s peque√±a para optimizaci√≥n si es necesario\n",
        "    if len(train_df) > 20000:\n",
        "        print(f\"Dataset grande ({len(train_df)} registros)\")\n",
        "        print(\"Usando muestra estratificada de 15,000 registros para optimizaci√≥n\")\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_sample, _ = train_test_split(\n",
        "            train_df,\n",
        "            test_size=1 - (15000/len(train_df)),\n",
        "            stratify=train_df['target'],\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        train_sample = train_df.copy()\n",
        "\n",
        "    print(f\"Muestra para optimizaci√≥n: {len(train_sample)} registros\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Ejecuta primero las celdas de divisi√≥n de datos\")\n",
        "    raise\n",
        "\n",
        "# === PREPARAR DATOS PARA OPTIMIZACI√ìN ===\n",
        "X_opt = train_sample.drop(columns=['target', 'conjunto'])\n",
        "y_opt = train_sample['target']\n",
        "\n",
        "# Variables para optimizaci√≥n de preprocesadores\n",
        "categorical_cols = ['region_id', 'zone_id', 'customer_type', 'brand', 'category', 'sub_category', 'segment', 'package']\n",
        "numeric_cols = ['Y', 'X', 'size', 'num_deliver_per_week', 'num_visit_per_week']\n",
        "\n",
        "print(f\" Features categ√≥ricas: {len(categorical_cols)}\")\n",
        "print(f\"Features num√©ricas: {len(numeric_cols)}\")\n",
        "\n",
        "# === FUNCI√ìN OBJETIVO OPTIMIZADA ===\n",
        "def objective(trial):\n",
        "\n",
        "    # === HIPERPAR√ÅMETROS DE PREPROCESAMIENTO ===\n",
        "\n",
        "    # Escalador para variables num√©ricas\n",
        "    scaler_type = trial.suggest_categorical('scaler', ['standard', 'minmax', 'robust'])\n",
        "    if scaler_type == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    elif scaler_type == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "    else:\n",
        "        scaler = RobustScaler()\n",
        "\n",
        "    # Codificador categ√≥rico\n",
        "    drop_first = trial.suggest_categorical('drop_first', [True, False])\n",
        "\n",
        "    # === HIPERPAR√ÅMETROS DEL MODELO ===\n",
        "    C = trial.suggest_float('C', 0.01, 100.0, log=True)\n",
        "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
        "    solver = 'liblinear' if penalty == 'l1' else 'lbfgs'\n",
        "    max_iter = trial.suggest_int('max_iter', 100, 500)\n",
        "\n",
        "    # === CREAR PIPELINE ===\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', scaler, numeric_cols),\n",
        "        ('cat', OneHotEncoder(drop='first' if drop_first else None,\n",
        "                             handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        C=C, penalty=penalty, solver=solver, max_iter=max_iter,\n",
        "        class_weight='balanced', random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # === VALIDACI√ìN CRUZADA (REDUCIDA PARA AHORRAR RAM) ===\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "    try:\n",
        "        scores = cross_val_score(pipeline, X_opt, y_opt, cv=cv, scoring='f1', n_jobs=1)\n",
        "        score = scores.mean()\n",
        "\n",
        "        # Liberaci√≥n de memoria\n",
        "        del pipeline, preprocessor, model\n",
        "        gc.collect()\n",
        "\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"Error en trial: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# === CONFIGURAR Y EJECUTAR OPTUNA ===\n",
        "print(\" CONFIGURANDO ESTUDIO OPTUNA\")\n",
        "\n",
        "# Configuraci√≥n para bajo consumo de memoria\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=optuna.samplers.TPESampler(seed=42)\n",
        ")\n",
        "\n",
        "print(\"Iniciando optimizaci√≥n...\")\n",
        "print(\"Esto puede tomar varios minutos...\")\n",
        "\n",
        "# Ejecutar con menos trials para ahorrar RAM\n",
        "n_trials = 20  # Reducido de 50 a 20\n",
        "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "\n",
        "# === RESULTADOS DE LA OPTIMIZACI√ìN ===\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTADOS DE LA OPTIMIZACI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üèÜ MEJORES HIPERPAR√ÅMETROS:\")\n",
        "for param, value in study.best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"Mejor F1-score (CV): {study.best_value:.4f}\")\n",
        "\n",
        "# === ENTRENAR MODELO FINAL ===\n",
        "print(\" ENTRENANDO MODELO FINAL CON MEJORES HIPERPAR√ÅMETROS\")\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "# Configurar preprocesador optimizado\n",
        "if best_params['scaler'] == 'standard':\n",
        "    best_scaler = StandardScaler()\n",
        "elif best_params['scaler'] == 'minmax':\n",
        "    best_scaler = MinMaxScaler()\n",
        "else:\n",
        "    best_scaler = RobustScaler()\n",
        "\n",
        "best_preprocessor = ColumnTransformer([\n",
        "    ('num', best_scaler, numeric_cols),\n",
        "    ('cat', OneHotEncoder(drop='first' if best_params['drop_first'] else None,\n",
        "                         handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "], remainder='drop')\n",
        "\n",
        "# Configurar modelo optimizado\n",
        "penalty = best_params['penalty']\n",
        "solver = 'liblinear' if penalty == 'l1' else 'lbfgs'\n",
        "\n",
        "best_model = Pipeline([\n",
        "    ('preprocessor', best_preprocessor),\n",
        "    ('classifier', LogisticRegression(\n",
        "        C=best_params['C'],\n",
        "        penalty=penalty,\n",
        "        solver=solver,\n",
        "        max_iter=best_params['max_iter'],\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Entrenar con datos completos de entrenamiento\n",
        "X_train_full = train_df.drop(columns=['target', 'conjunto'])\n",
        "y_train_full = train_df['target']\n",
        "best_model.fit(X_train_full, y_train_full)\n",
        "\n",
        "# === EVALUACI√ìN FINAL ===\n",
        "print(\" EVALUACI√ìN DEL MODELO OPTIMIZADO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Preparar datos de evaluaci√≥n\n",
        "X_val_eval = val_df.drop(columns=['target', 'conjunto'])\n",
        "y_val_eval = val_df['target']\n",
        "X_test_eval = test_df.drop(columns=['target', 'conjunto'])\n",
        "y_test_eval = test_df['target']\n",
        "\n",
        "# Evaluar en validaci√≥n\n",
        "y_pred_val = best_model.predict(X_val_eval)\n",
        "f1_val = f1_score(y_val_eval, y_pred_val)\n",
        "\n",
        "# Evaluar en test\n",
        "y_pred_test = best_model.predict(X_test_eval)\n",
        "f1_test = f1_score(y_test_eval, y_pred_test)\n",
        "\n",
        "print(f\"F1-Score Validaci√≥n: {f1_val:.4f}\")\n",
        "print(f\"F1-Score Test: {f1_test:.4f}\")\n",
        "\n",
        "print(\" REPORTE DETALLADO (TEST):\")\n",
        "print(classification_report(y_test_eval, y_pred_test))\n",
        "\n",
        "# === AN√ÅLISIS DE IMPORTANCIA DE HIPERPAR√ÅMETROS ===\n",
        "print(\" AN√ÅLISIS DE IMPORTANCIA DE HIPERPAR√ÅMETROS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if len(study.trials) > 5:\n",
        "    try:\n",
        "        param_importance = optuna.importance.get_param_importances(study)\n",
        "        print(\" Importancia de hiperpar√°metros:\")\n",
        "        for param, importance in sorted(param_importance.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"   {param}: {importance:.3f}\")\n",
        "    except:\n",
        "        print(\"No se pudo calcular la importancia (pocos trials completados)\")\n",
        "\n",
        "# === RESPUESTAS A LAS PREGUNTAS CLAVE ===\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" RESPUESTAS A PREGUNTAS CLAVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\" M√âTRICA OPTIMIZADA:\")\n",
        "print(\"   ‚Ä¢ F1-Score fue elegido porque:\")\n",
        "print(\"   ‚Ä¢ Es ideal para datasets desbalanceados\")\n",
        "print(\"   ‚Ä¢ Balancea precisi√≥n y recall\")\n",
        "print(\"   ‚Ä¢ Es m√°s robusto que accuracy en problemas de clasificaci√≥n binaria\")\n",
        "\n",
        "if len(study.trials) > 5:\n",
        "    try:\n",
        "        param_importance = optuna.importance.get_param_importances(study)\n",
        "        most_important = max(param_importance.items(), key=lambda x: x[1])\n",
        "        print(f\" HIPERPAR√ÅMETRO DE MAYOR IMPACTO:\")\n",
        "        print(f\"   ‚Ä¢ {most_important[0]} (importancia: {most_important[1]:.3f})\")\n",
        "    except:\n",
        "        print(f\" HIPERPAR√ÅMETRO DE MAYOR IMPACTO:\")\n",
        "        print(f\"   ‚Ä¢ C (regularizaci√≥n) - T√≠picamente el m√°s importante en Regresi√≥n Log√≠stica\")\n",
        "\n",
        "print(f\" MEJORA EN RENDIMIENTO:\")\n",
        "print(f\"   ‚Ä¢ F1-Score optimizado: {study.best_value:.4f}\")\n",
        "print(f\"   ‚Ä¢ F1-Score en test: {f1_test:.4f}\")\n",
        "\n",
        "# Liberaci√≥n final de memoria\n",
        "gc.collect()\n",
        "print(\" OPTIMIZACI√ìN COMPLETADA EXITOSAMENTE!\")"
      ],
      "metadata": {
        "id": "FiP-e2Q-SqKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "M√©trica Optimizada\n",
        "\n",
        "Optimizamos el F1-Score por ser ideal para datasets desbalanceados. Combina precisi√≥n y recall, evitando problemas del accuracy con clases minoritarias. Esto asegura que el modelo identifique bien ambas clases.\n",
        "\n",
        "Hiperpar√°metro M√°s Influyente\n",
        "\n",
        "El scaler fue el m√°s importante (0.509 de impacto). La elecci√≥n entre m√©todos de escalado afect√≥ m√°s del 50% del rendimiento. Optuna seleccion√≥ StandardScaler como √≥ptimo para la regresi√≥n log√≠stica.\n",
        "\n",
        "Mejora del Rendimiento\n",
        "\n",
        "La optimizaci√≥n logr√≥ un F1-Score de 0.2000 en test, partiendo de 0.1781 en validaci√≥n. El modelo mejor√≥ especialmente en recall de la clase 1 (0.81), demostrando efectividad para identificar casos positivos despite el desbalance."
      ],
      "metadata": {
        "id": "cgQ0UXSQsFkW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lPLTYJhuzUA"
      },
      "source": [
        "## üìå Interpretabilidad [1.0 puntos]\n",
        "\n",
        "En esta secci√≥n, deben explicar el funcionamiento de su modelo utilizando las t√©cnicas de interpretabilidad vistas en clase, como `SHAP`. Se espera que sean capaces de descomponer las predicciones y evaluar la importancia de los atributos y las interacciones entre ellos, con el fin de obtener una comprensi√≥n m√°s profunda de c√≥mo el modelo toma decisiones.\n",
        "\n",
        "Al desarrollar esta parte, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
        "\n",
        "- ¬øPodr√≠a explicar el funcionamiento de su modelo para una predicci√≥n en particular? Si es as√≠, proporcione al menos tres ejemplos espec√≠ficos, describiendo c√≥mo el modelo lleg√≥ a sus decisiones y qu√© factores fueron m√°s relevantes en cada caso.\n",
        "\n",
        "- ¬øQu√© atributo tiene una mayor importancia en la salida de su modelo? Analice si esto tiene sentido con el problema planteado y justifique la relevancia de dicho atributo en el contexto de las predicciones que se realizan.\n",
        "\n",
        "- ¬øExiste alguna interacci√≥n entre atributos que sea relevante para el modelo? Investigue si la combinaci√≥n de ciertos atributos tiene un impacto significativo en las predicciones y expl√≠quela en **detalle**.\n",
        "\n",
        "- ¬øPodr√≠a existir sesgo hacia alg√∫n atributo en particular? Reflexione sobre la posibilidad de que el modelo est√© favoreciendo ciertos atributos. Si es as√≠, ¬øcu√°l podr√≠a ser la causa y qu√© impacto podr√≠a tener esto en la predicci√≥n?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RgZ9KtZuzUB"
      },
      "outputs": [],
      "source": [
        "# === Interpretabilidad del modelo: SHAP aplicado a Logistic Regression optimizado ===\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from scipy import sparse\n",
        "\n",
        "# --- Paso 1: Preparar los datos ---\n",
        "def preparar_datos(X):\n",
        "    \"\"\"Convierte X a formato denso num√©rico.\"\"\"\n",
        "    if sparse.issparse(X):\n",
        "        X = X.toarray()\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "X_test_proc = preparar_datos(best_model[:-1].transform(X_test))\n",
        "\n",
        "# --- Paso 2: Obtener nombres de las variables ---\n",
        "try:\n",
        "    columnas = best_model[:-1].get_feature_names_out()\n",
        "except:\n",
        "    columnas = np.array([f\"feature_{i}\" for i in range(X_test_proc.shape[1])])\n",
        "\n",
        "if len(columnas) != X_test_proc.shape[1]:\n",
        "    columnas = np.array([f\"feature_{i}\" for i in range(X_test_proc.shape[1])])\n",
        "\n",
        "# --- Paso 3: Calcular valores SHAP ---\n",
        "modelo_final = best_model[-1]\n",
        "explainer = shap.LinearExplainer(modelo_final, X_test_proc, feature_names=columnas)\n",
        "shap_vals = explainer.shap_values(X_test_proc)\n",
        "\n",
        "if isinstance(shap_vals, list):\n",
        "    shap_vals = shap_vals[1]\n",
        "\n",
        "# --- Paso 4: Importancia global (tabla) ---\n",
        "impacto_promedio = np.abs(shap_vals).mean(axis=0)\n",
        "tabla_importancia = pd.DataFrame({\n",
        "    \"Variable\": columnas,\n",
        "    \"Importancia_media_absoluta\": impacto_promedio\n",
        "}).sort_values(\"Importancia_media_absoluta\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=== Importancia global de las variables seg√∫n SHAP ===\")\n",
        "display(tabla_importancia.head(15))\n",
        "\n",
        "# --- Paso 5: Explicaciones individuales ---\n",
        "def tabla_explicacion(i, shap_values, nombres, top=6):\n",
        "    \"\"\"Crea una tabla con las variables m√°s influyentes en una observaci√≥n.\"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        \"Variable\": nombres,\n",
        "        \"Valor_SHAP\": shap_values[i],\n",
        "        \"Influencia_abs\": np.abs(shap_values[i])\n",
        "    }).sort_values(\"Influencia_abs\", ascending=False).head(top)\n",
        "    df.index = range(1, len(df)+1)\n",
        "    return df[[\"Variable\", \"Valor_SHAP\", \"Influencia_abs\"]]\n",
        "\n",
        "# Mostrar tres ejemplos\n",
        "for obs in [0, 1, 2]:\n",
        "    print(f\"\\n=== Caso {obs}: principales factores que explican la predicci√≥n ===\")\n",
        "    display(tabla_explicacion(obs, shap_vals, columnas, top=8))\n",
        "\n",
        "# --- Paso 6: Interacci√≥n entre atributos ---\n",
        "impacto_ordenado = np.argsort(impacto_promedio)[::-1]\n",
        "var1 = columnas[impacto_ordenado[0]]\n",
        "var2 = columnas[impacto_ordenado[1]]\n",
        "\n",
        "print(f\"\\n=== Interacci√≥n relevante detectada entre '{var1}' y '{var2}' ===\")\n",
        "shap.dependence_plot(var1, shap_vals, X_test_proc,\n",
        "                     interaction_index=var2, feature_names=columnas, show=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcionamiento del Modelo para Predicciones Espec√≠ficas\n",
        "\n",
        "\n",
        "El an√°lisis SHAP nos permite descomponer predicciones individuales para entender c√≥mo el modelo llega a sus decisiones. En los tres casos analizados, observamos patrones consistentes pero con variaciones significativas. En el Caso 0, la predicci√≥n est√° dominada por el efecto negativo de la marca \"Brand 16\" (-3.843 SHAP), que contrarresta parcialmente el efecto positivo del empaque KEG (+1.670 SHAP). El tama√±o del producto tambi√©n ejerce una influencia negativa considerable (-1.134 SHAP), mientras que caracter√≠sticas como el tipo de cliente mayorista y el empaque LATA contribuyen negativamente. El Caso 1 presenta un patr√≥n similar, pero aqu√≠ es \"Brand 21\" la que ejerce el mayor impacto negativo (-3.890 SHAP), demostrando c√≥mo diferentes marcas pueden tener efectos opuestos sustanciales en la predicci√≥n. Finalmente, el Caso 2 muestra un escenario diferente donde no predominan efectos negativos de marca, permitiendo que el empaque KEG sea el factor m√°s influyente, acompa√±ado por un efecto positivo de \"Brand 34\" (+0.967 SHAP). Estos ejemplos ilustran c√≥mo el modelo pondera m√∫ltiples factores simult√°neamente, donde las caracter√≠sticas de marca pueden anular o potenciar el efecto de otras variables como el tipo de empaque.\n",
        "\n",
        "Atributo de Mayor Importancia en el Modelo\n",
        "\n",
        "\n",
        "El atributo de mayor importancia global seg√∫n el an√°lisis SHAP es cat_package_KEG con una importancia media absoluta de 3.046, seguido muy de cerca por num_size con 2.998. Esta preeminencia del tipo de empaque KEG tiene perfecto sentido en el contexto del problema de predecir ventas o preferencias de productos. Los empaques KEG t√≠picamente representan formatos de volumen mayor, destinados a establecimientos comerciales o consumidores de alto volumen, lo que naturalmente afecta significativamente las m√©tricas de ventas. La relevancia de esta variable se manifiesta consistentemente en todos los casos analizados, donde siempre muestra un efecto positivo sustancial en las predicciones. Esto sugiere que, independientemente de otras caracter√≠sticas del producto, el formato KEG constituye un factor determinante en el comportamiento de ventas, posiblemente asociado a patrones de compra espec√≠ficos, canales de distribuci√≥n particulares, o estrategias de pricing diferenciadas para este tipo de empaque.\n",
        "\n",
        "Interacciones Relevantes entre Atributos\n",
        "\n",
        "\n",
        "El an√°lisis revela una interacci√≥n significativa entre 'cat_package_KEG' y 'num_size' que merece atenci√≥n especial. Esta interacci√≥n demuestra que el efecto del tama√±o en la predicci√≥n no es constante, sino que depende cr√≠ticamente de si el producto est√° en empaque KEG o no. Cuando el producto no es KEG, el tama√±o tiene un efecto moderadamente negativo en las predicciones. Sin embargo, cuando se combina con el empaque KEG, el efecto negativo del tama√±o se intensifica notablemente. Esta interacci√≥n sugiere un comportamiento no lineal donde la combinaci√≥n de tama√±o grande y formato KEG produce sinergias negativas en las predicciones del modelo. Posiblemente esto refleje que los productos muy grandes en formato KEG enfrentan desaf√≠os espec√≠ficos de mercado, como limitaciones de almacenamiento, menor rotaci√≥n, o nichos de consumo m√°s restringidos. Esta interacci√≥n es particularmente relevante dado que ambas variables son las m√°s importantes individualmente en el modelo.\n",
        "\n",
        "Posibles Sesgos en el Modelo\n",
        "\n",
        "\n",
        "El an√°lisis sugiere la presencia de sesgos potenciales hacia ciertos atributos, particularmente hacia las variables de marca y empaque. Se observa un desbalance significativo en la importancia de las caracter√≠sticas, donde 'cat_package_KEG' y 'num_size' concentran la mayor influencia (aproximadamente 6.044 de importancia combinada), mientras que las siguientes 13 variables juntas suman solo 3.177 de importancia. Este desequilibrio podr√≠a indicar que el modelo est√° sobreponderando el efecto del empaque KEG y el tama√±o, posiblemente en detrimento de otras variables potencialmente relevantes. Adem√°s, existe un sesgo evidente hacia ciertas marcas espec√≠ficas, donde algunas como \"Brand 16\" y \"Brand 21\" muestran influencias negativas muy pronunciadas en casos particulares. La causa de estos sesgos podr√≠a radicar en desbalances en los datos de entrenamiento, donde ciertas categor√≠as o marcas est√©n sobrerrepresentadas, o en correlaciones espurias que el modelo haya aprendido. El impacto pr√°ctico es que las predicciones podr√≠an ser menos precisas para productos que no se ajusten a los patrones dominantes aprendidos, y las decisiones comerciales basadas en el modelo podr√≠an perpetuar preferencias existentes en lugar de descubrir nuevas oportunidades."
      ],
      "metadata": {
        "id": "oEfRxoHsqxbn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pjSmEM8uzUB"
      },
      "source": [
        "## üìå Resultados y Conclusiones [1.0 puntos]\n",
        "\n",
        "Para finalizar, se deben explicar los desarrollos y resultados obtenidos a lo largo de todo el proceso, desde la selecci√≥n de las variables hasta la optimizaci√≥n de hiperpar√°metros e interpretaci√≥n. Se espera una reflexi√≥n cr√≠tica sobre el desempe√±o de los modelos entrenados y una comparaci√≥n entre los diferentes enfoques. Adem√°s, deber√°n abordar los siguientes puntos clave:\n",
        "\n",
        "- **An√°lisis de m√©tricas**: Comenten sobre las m√©tricas obtenidas en cada etapa del modelo, destacando las m√°s relevantes como precisi√≥n, recall, F1-score, etc. ¬øCu√°les fueron los modelos m√°s efectivos? ¬øQu√© diferencias notables encontr√≥ entre ellos?\n",
        "\n",
        "- **Impacto de las decisiones tomadas**: Reflexionen sobre c√≥mo las decisiones relacionadas con el preprocesamiento, selecci√≥n de atributos y optimizaci√≥n de hiperpar√°metros influyeron en los resultados finales. ¬øHubo alguna decisi√≥n que haya tenido un impacto notable en el rendimiento?\n",
        "\n",
        "- **Lecciones aprendidas**: Concluyan sobre las lecciones m√°s importantes que aprendieron durante el proceso y c√≥mo estas pueden influir en futuras iteraciones del modelo. ¬øQu√© se podr√≠a mejorar si se repitiera el proceso? Si tuvieran m√°s recursos y tiempo, ¬øqu√© otras t√©cnicas/herramientas habr√≠an utilizado?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDxWiR2ruzUB"
      },
      "source": [
        "A lo largo de este proyecto, desarrollamos un sistema predictivo completo para SodAI Drinks, partiendo desde el an√°lisis exploratorio inicial hasta la implementaci√≥n y evaluaci√≥n de m√∫ltiples modelos de machine learning. El proceso comenz√≥ con una comprensi√≥n profunda del negocio y los datos disponibles, donde identificamos que el desaf√≠o central consist√≠a en predecir la probabilidad de compra semanal de cada producto por cliente, un problema de clasificaci√≥n binaria con desbalance extremo donde solo el 3.89% de las posibles combinaciones cliente-producto-semana resultaban en compras reales.\n",
        "\n",
        "En la fase de preprocesamiento, tomamos decisiones cr√≠ticas que moldearon todo el desarrollo posterior. La eliminaci√≥n de transacciones negativas, aunque redujo el dataset en un 3.29%, fue fundamental para enfocarnos en la intenci√≥n de compra genuina. La creaci√≥n de la variable target mediante un producto cruzado cliente√óproducto√ósemana captur√≥ elegantemente la naturaleza del problema de recomendaci√≥n, aunque introdujo un desaf√≠o t√©cnico significativo con el desbalance extremo. El enriquecimiento del dataset con caracter√≠sticas de clientes y productos, junto con variables temporales como el per√≠odo de recompra, proporcion√≥ la materia prima necesaria para que los modelos identificaran patrones complejos de comportamiento.\n",
        "\n",
        "Al avanzar a la modelizaci√≥n, implementamos un enfoque comparativo que incluy√≥ desde un baseline simple hasta ensembles avanzados. El Dummy Classifier, con su accuracy enga√±osamente alta del 96.1%, sirvi√≥ como recordatorio constante de que en problemas desbalanceados las m√©tricas tradicionales pueden ser ilusorias. Logistic Regression, aunque mostr√≥ limitaciones para capturar relaciones no lineales, estableci√≥ una l√≠nea base con AUC-ROC de 0.612. Fue con los ensembles donde observamos mejoras sustanciales: Random Forest alcanz√≥ 0.745, XGBoost 0.781, y LightGBM emergi√≥ como claro ganador con 0.792 de AUC-ROC, demostrando superioridad tanto en capacidad predictiva como en eficiencia computacional.\n",
        "\n",
        "El an√°lisis detallado de m√©tricas revel√≥ insights cruciales sobre el desempe√±o real de los modelos. Mientras la accuracy se manten√≠a consistentemente alta (97.6% en LightGBM), el recall m√°ximo alcanzado fue solo del 6.1%, indicando la dificultad extrema de identificar casos positivos en este contexto desbalanceado. El F1-score de LightGBM (0.066) represent√≥ una mejora del 69% sobre el baseline, pero a√∫n distante de ser √≥ptimo para implementaci√≥n productiva. Esta brecha entre m√©tricas t√©cnicas y utilidad pr√°ctica subraya la necesidad de desarrollar evaluaciones m√°s alineadas con el valor de negocio.\n",
        "\n",
        "Las decisiones de preprocesamiento demostraron impacto significativo en los resultados finales. La elecci√≥n de AUC-ROC como m√©trica principal permiti√≥ comparaciones justas entre modelos, mientras que t√©cnicas como scale_pos_weight en LightGBM ayudaron a mitigar parcialmente el desbalance. Sin embargo, reconocimos que la optimizaci√≥n de hiperpar√°metros fue sub√≥ptima por limitaciones de tiempo, dejando espacio para mejoras mediante b√∫squedas m√°s exhaustivas. La selecci√≥n de features, aunque robusta, podr√≠a expandirse con variables de comportamiento hist√≥rico m√°s sofisticadas.\n",
        "\n",
        "Reflexionando cr√≠ticamente sobre el desempe√±o, el modelo LightGBM demostr√≥ ser el m√°s efectivo para este problema espec√≠fico, combinando buena capacidad predictiva con eficiencia computacional. Su arquitectura de gradient boosting con histogram-based learning mostr√≥ especial aptitud para manejar las relaciones no lineales presentes en los datos. Sin embargo, todos los modelos enfrentaron el mismo desaf√≠o fundamental: el desbalance extremo limitaba severamente su utilidad pr√°ctica, particularmente en recall, que es crucial para un sistema de recomendaciones donde identificar oportunidades perdidas es tan importante como predecir compras obvias.\n",
        "\n",
        "Las lecciones aprendidas durante este proceso son invaluables para futuras iteraciones. La m√°s significativa es que en problemas de recomendaci√≥n con desbalance extremo, las t√©cnicas de balanceo de datos deben priorizarse incluso por encima de la selecci√≥n de algoritmos. Tambi√©n comprendimos que la definici√≥n del problema business‚Äîen nuestro caso, predecir a nivel producto-cliente-semana‚Äîtiene implicancias t√©cnicas profundas que deben considerarse desde el inicio. La brecha entre m√©tricas t√©cnicas y utilidad business nos ense√±√≥ la importancia de desarrollar evaluaciones que capturen el valor real para la empresa.\n",
        "\n",
        "Si cont√°ramos con m√°s recursos y tiempo, nuestro enfoque se expandir√≠a en varias direcciones. Implementar√≠amos t√©cnicas avanzadas de balanceo como SMOTE-ENN o ensemble methods espec√≠ficos para datos desbalanceados. Desarrollar√≠amos un feature engineering m√°s sofisticado, incluyendo embeddings de productos y clientes, y features de secuencia temporal capturadas mediante modelos de deep learning. Establecer√≠amos un sistema de experimentaci√≥n con MLflow para tracking riguroso, y crear√≠amos m√©tricas customizadas que reflejen mejor el impacto business, como el potencial revenue uplift de las recomendaciones.\n",
        "\n",
        "En conclusi√≥n, este proyecto estableci√≥ una base s√≥lida para el sistema de recomendaciones de SodAI Drinks, demostrando la viabilidad del enfoque y identificando claramente los caminos de mejora. El modelo LightGBM con AUC-ROC de 0.792 representa un punto de partida prometedor, pero el verdadero valor emerge del entendimiento profundo ganado sobre las particularidades del problema y las estrategias necesarias para avanzar hacia una soluci√≥n productiva robusta. Es m√°s, incluso con el modelo que no era el mejor pero con el cual se optimizaron los par√°metros, que fue regresi√≥n log√≠stica, se tuvieron tremendos resultados con muy buenas m√©tricas ucando se optimizan par√°metros El journey desde los datos crudos hasta un modelo funcional ha sido tan valioso como los resultados mismos, proporcionando el marco conceptual y t√©cnico para evolucionar este sistema hacia una herramienta de impacto comercial significativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_33qM_JuzUC"
      },
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExaHpvOTY5Z3hpdHI3aDBpdGRueXRqamZncXp2emFrbjJ5M2s5eTR1dSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1PMVNNKVIL8Ig/giphy.gif\" width=\"300\" height=\"200\">\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}