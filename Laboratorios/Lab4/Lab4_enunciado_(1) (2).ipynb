{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac9155b9f5e04400957a6f8bb3f6610c",
        "deepnote_cell_type": "markdown",
        "id": "2v2D1coL7I8i"
      },
      "source": [
        "<h1><center>Laboratorio 4: La solicitud de Mathias ü§ó</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d3d6f6d405c54dbe985a5f4b3e4f9120",
        "deepnote_cell_type": "markdown",
        "id": "YxdTmIPD7L_x"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "851a7788e8214942863cbd4099064ab2",
        "deepnote_cell_type": "markdown",
        "id": "Y2Gyrj-x7N2L"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Maximiliano Farren\n",
        "- Nombre de alumno 2: Sebasti√°n Rubina\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f23a189afdec4e198683308db70e43b7",
        "deepnote_cell_type": "markdown",
        "id": "jQ9skYc57Pxi"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/MacFarren/Lab-Farren-Rubina)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5318f41cda64d4290a7a548956ed725",
        "deepnote_cell_type": "markdown",
        "id": "1M4PoEWm7S80"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers.\n",
        "- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.\n",
        "- Familiarizarse con plotly.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "858df483d9e64780a21674afed1d34b8",
        "deepnote_cell_type": "markdown",
        "id": "SuMbiyQZG2Cc"
      },
      "source": [
        "## Descripci√≥n del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "403ffe48ec994afda4b91e670a08d0ef",
        "deepnote_cell_type": "markdown",
        "id": "QZsNO4rUrqCz"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0303baa17d4546feae8c9b88c58470bf",
        "deepnote_cell_type": "markdown",
        "id": "2o0MPuk8rqCz"
      },
      "source": [
        "En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Mathias, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Mathias, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.\n",
        "\n",
        "Mathias encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.\n",
        "\n",
        "A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e78cb41b144041af98928ab26dcfdaa9",
        "deepnote_cell_type": "markdown",
        "id": "hs4KKWF1Hdpo"
      },
      "source": [
        "## Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "95a5533cfd6d49cfb9afc111c44d224f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1714107106552,
        "id": "a4YpMafirqC0",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import time\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "from sklearn.metrics import silhouette_score #dont used\n",
        "\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "acbeab32db6146678e75448dddf43da8",
        "deepnote_cell_type": "markdown",
        "id": "UQOXod4gHhSq"
      },
      "source": [
        "## 1. Estudio de Performance üìà [10 Puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "704b56b978254ad3ae12cdbf58f4832d",
        "deepnote_cell_type": "markdown",
        "id": "Gn5u5ICkrqC2"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/23/b7/6e/23b76e9e77e63c0eec1a7b28372369e3.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d35fbdcc5ef045d6a2822622f0714179",
        "deepnote_cell_type": "markdown",
        "id": "y4Z0jTjtrqC2"
      },
      "source": [
        "Don Mathias les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.\n",
        "\n",
        "Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.\n",
        "\n",
        "Uno de los requisitos establecidos por Mathias es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />\n",
        "\n",
        "En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37580aab6cef4238a8ce42c50a6d35de",
        "deepnote_cell_type": "markdown",
        "id": "maCUNAvZrqC2"
      },
      "source": [
        "Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:\n",
        "1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]\n",
        "2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]\n",
        "3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]\n",
        "\n",
        "\n",
        "> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.\n",
        "\n",
        "> ‚ùó Recuerde que es obligatorio el uso de plotly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7f7c25e366754595b13fc2e8116f65a0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1714107108441,
        "id": "i0IZPGPOrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Configuracion\n",
        "n_samples = 5000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y51s6f_UtIkc"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "643d6b35af5541358f481fda4d3fc51f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 267,
        "execution_start": 1714108733824,
        "id": "CO3JFqezrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "def plot_scatter(x, y, color):\n",
        "#grafico dispersion\n",
        "    fig = px.scatter(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        color=color.astype(str),\n",
        "        labels={\"x\": \"Eje X\", \"y\": \"Eje Y\", \"color\": \"Cluster\"},\n",
        "        title=\"Gr√°fico de dispersi√≥n\"\n",
        "    )\n",
        "    fig.update_traces(marker=dict(size=6, line=dict(width=0.5, color=\"black\")))\n",
        "    fig.update_layout(\n",
        "        width=600,\n",
        "        height=400,\n",
        "        title_x=0.5,\n",
        "        plot_bgcolor=\"white\"\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18q6CfVvEgXw"
      },
      "outputs": [],
      "source": [
        "def compare_clusterings(data_sets):\n",
        "    modelos = {\n",
        "        \"KMeans\": lambda n: KMeans(n_clusters=n, random_state=42),\n",
        "        \"GMM\": lambda n: GaussianMixture(n_components=n, random_state=42),\n",
        "        \"WARD\": lambda n: AgglomerativeClustering(n_clusters=n, linkage=\"ward\"),\n",
        "        \"DBSCAN\": lambda n: DBSCAN(eps=0.5, min_samples=5)\n",
        "    }\n",
        "\n",
        "    # Crear t√≠tulos para cada subplot usando numpy\n",
        "    dataset_names = np.array(list(data_sets.keys()))\n",
        "    model_names = np.array(list(modelos.keys()))\n",
        "\n",
        "    # Crear grid de √≠ndices usando numpy\n",
        "    dataset_indices = np.repeat(np.arange(len(dataset_names)), len(model_names))\n",
        "    model_indices = np.tile(np.arange(len(model_names)), len(dataset_names))\n",
        "\n",
        "    # Inicializar t√≠tulos de subplots\n",
        "    subplot_titles = np.array([model_names[j] for j in model_indices])\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=4,\n",
        "        subplot_titles=subplot_titles.tolist(),\n",
        "        vertical_spacing=0.08,\n",
        "        horizontal_spacing=0.05\n",
        "    )\n",
        "\n",
        "    # Funci√≥n para procesar un dataset-modelo\n",
        "    def process_clustering(idx):\n",
        "        i_dataset = dataset_indices[idx]\n",
        "        i_model = model_indices[idx]\n",
        "\n",
        "        dataset_name = dataset_names[i_dataset]\n",
        "        model_name = model_names[i_model]\n",
        "        model_func = modelos[model_name]\n",
        "\n",
        "        dataset = data_sets[dataset_name]\n",
        "        X = dataset[\"x\"]\n",
        "        n_clusters = dataset[\"n_cluster\"]\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if model_name == \"GMM\":\n",
        "            model = model_func(n_clusters)\n",
        "            labels = model.fit_predict(X)\n",
        "        elif model_name == \"DBSCAN\":\n",
        "            model = model_func(None)\n",
        "            labels = model.fit_predict(X)\n",
        "        else:\n",
        "            model = model_func(n_clusters)\n",
        "            labels = model.fit_predict(X)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        # Calcular silhouette si hay m√°s de 1 cluster\n",
        "        if len(np.unique(labels)) > 1:\n",
        "            sil = silhouette_score(X, labels)\n",
        "        else:\n",
        "            sil = -1\n",
        "\n",
        "        # Agregar puntos al subplot\n",
        "        row_pos = i_dataset + 1\n",
        "        col_pos = i_model + 1\n",
        "\n",
        "        fig.add_scatter(\n",
        "            x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                color=labels,\n",
        "                colorscale='viridis',\n",
        "                size=4,\n",
        "                line=dict(width=0.5, color='black')\n",
        "            ),\n",
        "            showlegend=False,\n",
        "            row=row_pos,\n",
        "            col=col_pos\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'index': idx,\n",
        "            'model': model_name,\n",
        "            'time': elapsed,\n",
        "            'silhouette': sil\n",
        "        }\n",
        "\n",
        "    # Procesar todos los clustering usando numpy vectorize\n",
        "    indices = np.arange(len(dataset_indices))\n",
        "    vectorized_process = np.vectorize(process_clustering, otypes=[object])\n",
        "    results = vectorized_process(indices)\n",
        "\n",
        "    # Actualizar t√≠tulos usando numpy operations\n",
        "    def update_title(result):\n",
        "        idx = result['index']\n",
        "        if idx < len(fig.layout.annotations):\n",
        "            fig.layout.annotations[idx].text = f\"{result['model']}<br>{result['time']:.2f} [s] | s: {result['silhouette']:.2f}\"\n",
        "\n",
        "    # Aplicar actualizaciones de t√≠tulo vectorizadamente\n",
        "    np.vectorize(update_title, otypes=[type(None)])(results)\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=900, width=1200,\n",
        "        title_text=\"Comparaci√≥n de tiempos de ejecuci√≥n por t√©cnica\",\n",
        "        title_x=0.5,\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    # Limpiar ejes\n",
        "    fig.update_xaxes(showticklabels=False, showgrid=False)\n",
        "    fig.update_yaxes(showticklabels=False, showgrid=False)\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cr-9_uWqEnH0",
        "outputId": "eaab5877-d0d9-4b8e-a6ce-74d931398461"
      },
      "outputs": [],
      "source": [
        "# Ejecutar comparaciones para diferentes tama√±os usando numpy vectorizado\n",
        "n_samples_array = np.array([1000, 5000, 10000])\n",
        "\n",
        "# Funci√≥n simple para ejecutar una comparaci√≥n\n",
        "def run_single_comparison(n):\n",
        "    print(f\"\\n=== Comparaci√≥n con n_samples = {n} ===\")\n",
        "    data_sets = create_data(n)\n",
        "    compare_clusterings(data_sets)\n",
        "\n",
        "# Ejecutar todas las comparaciones usando numpy.vectorize correctamente\n",
        "def execute_comparisons(n_samples_array):\n",
        "    vectorized_run = np.vectorize(run_single_comparison, otypes=[type(None)])\n",
        "    vectorized_run(n_samples_array)\n",
        "\n",
        "execute_comparisons(n_samples_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx3DNSigFgRG"
      },
      "source": [
        "Es posible observar diferencias entre los algorimos ejecutados.\n",
        "\n",
        "K-Means muestra un rendimiento similar en todos los datos, en \"blobs\" en el que los datos esfericos se identifican como naturales, en \"moons\", presenta limitaciones, donde la forma no convexa de los clusters hace que no se logre dividir correctamente.\n",
        "\n",
        "GMM presenta un comportamiento similar al K-Means en debido a que est√° basado en distribuciones gaussianas. Aun as√≠ brinda un mayor margen de flexibilidad, al permitir clusters con diferentes formas de covarianzas. La ventaja principal ser√≠a que proporciona probabilidades de pertenencia suaves en lugar de intervenciones r√≠gidas.\n",
        "\n",
        "Ward ofrece resultados distintos seg√∫n el tipo de datos. Parte de un buen rendimiento en conjuntos de datos con estructura jer√°rquica natural, pero genera clusters desbalanceados si la estructura de los datos no coincide con el tipo de estrategia  que estamos buscando. El hecho de que minimice la varianza intra-cluster hace que sea muy eficiente para los grupos compactos y bien separados.\n",
        "\n",
        "DBSCAN posee una capacidad de poder manejar clusters de cualquier forma y detectar el ruido. Es bueno en \"moons\", donde los clusters no son convexos. Su desempe√±o es sensible a los par√°metros y tiene problemas al trabajar con densidades diferentes.\n",
        "\n",
        "Los tiempos de ejecuci√≥n reflejan una tendencia creciente seg√∫n el tama√±o de los datasets. K-Means y GMM presentan comportamientos temporales similares, siendo GMM  m√°s lento por la moda iterativa del algoritmo EM, mientras que Ward presenta tiempos intermedios. Se da cuenta que DBSCAN puede ser m√°s lento o m√°s r√°pido, dependiendo de la forma en que distribuyen los datos y los par√°metros que se han seleccionado.\n",
        "\n",
        "Los valores de silhouette aportan informaci√≥n acerca de la calidad de la segmentaci√≥n. K-Means y GMM suelen dar scores altos, en datasets, cuando los clusters est√°n bien separados, y son de forma esf√©rica. Ward, por su parte, podr√≠a llegar a obtener scores relativamente buenos en aquellos casos en los que la estructura jer√°rquica y la cantidad de puntos de cada cluster es la adecuada. DBSCAN puede ser bueno en aquellos casos en los que hay clusters con formas complejas, aunque puede llegar a tener scores m√°s bajos a causa de su capacidad de identificar ruido en aquellas intersecciones que quedan como puntos no asignados.\n",
        "\n",
        "Con el aumento del tama√±o de muestra de 1.000 a 10.000 observaciones, todos los algoritmos mantienen su conducta relativa pero con diferencias asociadas a la escalabilidad. K-Means presenta la mejor escalabilidad lineal, mientras que Ward muestra complejidad cuadr√°tica que se manifiesta m√°s expl√≠citamente con los datasets grandes. Por otro lado, DBSCAN presenta escalabilidad dependiente de la implementaci√≥n del √≠ndice espacial que se utilice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13c5cb8067d9415f83b3d497954a437a",
        "deepnote_cell_type": "markdown",
        "id": "3mCbZc86rqC6"
      },
      "source": [
        "## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd6e991646b44f50a4b13f01d1542415",
        "deepnote_cell_type": "markdown",
        "id": "JI33m5jbrqC6"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media4.giphy.com/media/v1.Y2lkPTZjMDliOTUyb3B5Y3BtbTZwMnB0ZXRyejFpanJkNDl5cGhoeWlsc2k5bGx1MTUwYSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/l4FARHkIFJReGSy2c/giphy.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5742dfbd5a2e43778ff250436bab1005",
        "deepnote_cell_type": "markdown",
        "id": "h5k24znirqC7"
      },
      "source": [
        "Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Mathias* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:\n",
        "\n",
        "- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)\n",
        "- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)\n",
        "- *Age*: Edad actual de los pasajeros\n",
        "- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)\n",
        "- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)\n",
        "- *Flight distance*: Distancia del vuelo de este viaje\n",
        "- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)\n",
        "- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada\n",
        "- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea\n",
        "- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta\n",
        "- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida\n",
        "- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea\n",
        "- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento\n",
        "- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo\n",
        "- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo\n",
        "- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas\n",
        "- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje\n",
        "- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in\n",
        "- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo\n",
        "- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza\n",
        "- *Departure Delay in Minutes*: Minutos de retraso en la salida\n",
        "- *Arrival Delay in Minutes*: Minutos de retraso en la llegada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoIFHpw5xCW"
      },
      "source": [
        "En consideraci√≥n de lo anterior, realice las siguientes tareas:\n",
        "\n",
        "0. Ingeste el dataset a su ambiente de trabajo.\n",
        "\n",
        "1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]\n",
        "\n",
        "2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]\n",
        "\n",
        "3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]\n",
        "\n",
        "4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]\n",
        "\n",
        "5. De acuerdo con los resultados obtenidos en 4, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6tcVBCtxxS"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3kEG6xNosPf"
      },
      "outputs": [],
      "source": [
        "#conda install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qo6xeZH3jz6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "pzHTZ17xveU_",
        "outputId": "ab7a079f-f668-4096-91fd-676b4d3c13e6"
      },
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "# Subir archivo para colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Cargar los archivos\n",
        "df = pd.read_parquet(\"aerolineas_lucer.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgjKv2mVGHQX",
        "outputId": "abf2221f-047b-4e10-9144-82e5f08d1e16"
      },
      "outputs": [],
      "source": [
        "# Solo variables num√©ricas\n",
        "df_num = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
        "df_num = df_num.drop(columns=['id'])\n",
        "\n",
        "print(\"Variables num√©ricas seleccionadas:\")\n",
        "print(df_num.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gH3pXImosPh"
      },
      "source": [
        "\n",
        "Si las variables est√°n en distintas escalas, ser√° necesario aplicar un escalado antes de clustering, ya que, no es posible trabajar cuando las vairbales se encuentran en diferentes rangos, unidades de medida o escala propiamente tal, los algoritmos asignan pesos diferentes a los reales, ya que algunos son sensibles a las magnitudes de los valores, si no se realiza esto, las variabkles pueden tomar ponderaciones que no son las correspondientes al c√°lculo y para su posterior an√°lisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rga8o_S3GnwQ",
        "outputId": "a6f4c2e0-7f21-4fb1-cb89-27362ad63bae"
      },
      "outputs": [],
      "source": [
        "# Histogramas de las distribuciones\n",
        "\n",
        "\n",
        "# Verificar que df_num existe y tiene datos\n",
        "if 'df_num' not in locals() or df_num.empty:\n",
        "    print(\"Error: df_num no est√° definido. Ejecuta primero las celdas de carga de datos.\")\n",
        "else:\n",
        "    # Obtener n√∫mero de columnas y calcular filas y columnas para subplots\n",
        "    n_cols = len(df_num.columns)\n",
        "    cols_per_row = 3\n",
        "    n_rows = math.ceil(n_cols / cols_per_row)\n",
        "\n",
        "    # Crear subplots\n",
        "    fig = make_subplots(\n",
        "        rows=n_rows,\n",
        "        cols=cols_per_row,\n",
        "        subplot_titles=df_num.columns.tolist(),\n",
        "        vertical_spacing=0.08,\n",
        "        horizontal_spacing=0.05\n",
        "    )\n",
        "\n",
        "    # Agregar histogramas para cada variable\n",
        "    for idx, col in enumerate(df_num.columns):\n",
        "        row = (idx // cols_per_row) + 1\n",
        "        col_pos = (idx % cols_per_row) + 1\n",
        "\n",
        "        fig.add_histogram(\n",
        "            x=df_num[col],\n",
        "            name=col,\n",
        "            row=row,\n",
        "            col=col_pos,\n",
        "            nbinsx=30,\n",
        "            showlegend=False\n",
        "        )\n",
        "\n",
        "    # Actualizar layout\n",
        "    fig.update_layout(\n",
        "        height=300 * n_rows,\n",
        "        width=1200,\n",
        "        title_text=\"Distribuciones de Variables Num√©ricas\",\n",
        "        title_x=0.5\n",
        "    )\n",
        "\n",
        "    # Mostrar figura con manejo de errores\n",
        "    try:\n",
        "        fig.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error al mostrar la figura: {e}\")\n",
        "        print(\"Intentando mostrar como HTML...\")\n",
        "        fig.show(renderer=\"browser\")\n",
        "\n",
        "    # Mostrar estad√≠sticas descriptivas\n",
        "    print(\"\\nEstad√≠sticas descriptivas de las variables num√©ricas:\")\n",
        "    print(df_num.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUXXaTtAosPj"
      },
      "source": [
        "Variables continuas:\n",
        "- Age: Presenta una distribuci√≥n aproximadamente normal con ligero sesgo hacia edades m√°s j√≥venes. La mayor√≠a de pasajeros est√°n entre 25-60 a√±os, con media alrededor de 39 a√±os.\n",
        "- Flight Distance: Muestra una distribuci√≥n sesgada hacia la derecha, con muchos vuelos cortos y pocos vuelos de larga distancia. La mayor√≠a est√°n bajo 2000 millas.\n",
        "- Departure Delay in Minutes: Distribuci√≥n con mayor√≠a sin retrasos y una cola larga hacia la derecha. Algunos vuelos tienen retrasos extremos.\n",
        "- Arrival Delay in Minutes: Similar al retraso de salida, concentrada en valores bajos con cola hacia valores altos.\n",
        "\n",
        "Todas las variables de satisfacci√≥n muestran patrones similares (escala 1-5): distribuciones discretas en escala Likert (1-5), tendencia hacia calificaciones medias-altas (3-4), algunas con mayor concentraci√≥n en valores espec√≠ficos.\n",
        "\n",
        "De esta forma, es necesario aplicar escalado ya que: las variables tienen rangos muy diferentes (Age: 7-85, Flight Distance: 31-4983, delays: 0-1592), las unidades son diferentes (a√±os, millas, minutos, escalas ordinales), sin escalado, las variables con mayor rango dominar√≠an el an√°lisis de clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uvCqPvuLLWV"
      },
      "outputs": [],
      "source": [
        "# Variables continuas reales\n",
        "numeric_continuous = [\n",
        "    \"Age\",\n",
        "    \"Flight Distance\",\n",
        "    \"Departure Delay in Minutes\",\n",
        "    \"Arrival Delay in Minutes\"\n",
        "]\n",
        "\n",
        "# Variables ordinales (escalas 1 a 5 de satisfacci√≥n)\n",
        "numeric_ordinal = [\n",
        "    \"Inflight wifi service\",\n",
        "    \"Departure/Arrival time convenient\",\n",
        "    \"Ease of Online booking\",\n",
        "    \"Gate location\",\n",
        "    \"Food and drink\",\n",
        "    \"Online boarding\",\n",
        "    \"Seat comfort\",\n",
        "    \"Inflight entertainment\",\n",
        "    \"On-board service\",\n",
        "    \"Leg room service\",\n",
        "    \"Baggage handling\",\n",
        "    \"Checkin service\",\n",
        "    \"Inflight service\",\n",
        "    \"Cleanliness\"\n",
        "]\n",
        "\n",
        "# Variable a descartar\n",
        "to_drop = [\"id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJWVR5bwBTw"
      },
      "source": [
        "Es importante notar que debido a la naturaleza del problema es que se ha decididoconsiderar todas las variables que sean float64 o int64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6hdLWqGvnQ"
      },
      "source": [
        "Si las variables est√°n en distintas escalas,  ser√° necesario aplicar un escalado antes de clustering, ya que, no es posible trabajar cuando las vairbales se encuentran en diferentes rangos,  unidades de medida o escala propiamente tal, los algoritmos asignan pesos diferentes a los reales, ya que algunos son sensibles a las magnitudes de los valores, si no se realiza esto, las variabkles pueden tomar ponderaciones que no son las correspondientes al c√°lculo y para su posterior an√°lisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "pWQghKHXFuZ7",
        "outputId": "592a5b47-2974-43b8-ab61-c94e0ef25981"
      },
      "outputs": [],
      "source": [
        "# Correlograma con Plotly\n",
        "corr = df_num.corr()\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=corr.values,\n",
        "    x=list(corr.columns),\n",
        "    y=list(corr.columns),\n",
        "    annotation_text=corr.round(2).values,\n",
        "    colorscale=\"RdBu\",\n",
        "    showscale=True\n",
        ")\n",
        "fig.update_layout(title=\"Matriz de correlaci√≥n entre variables num√©ricas\", title_x=0.5, width=800, height=800)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pCfr3VqosPk"
      },
      "source": [
        "Variables altamente correlacionadas identificadas:\n",
        "- Variables de servicios de vuelo (wifi, entretenimiento, comida, asientos) presentan correlaciones moderadas a altas entre s√≠ (0.3-0.6)\n",
        "- Variables de servicios aeroportuarios (check-in, boarding, gate location) tambi√©n est√°n correlacionadas (0.4-0.7)\n",
        "- Los delays de salida y llegada muestran correlaci√≥n alta como es esperado (>0.9)\n",
        "- Algunas variables de satisfacci√≥n est√°n fuertemente correlacionadas entre s√≠\n",
        "\n",
        "Para reducir a 4 variables manteniendo la m√°xima informaci√≥n, seleccionamos variables que: Tengan baja correlaci√≥n entre s√≠ para evitar redundancia, representen diferentes aspectos del servicio de la aerol√≠nea, incluyan tanto variables continuas como de satisfacci√≥n, mantengan la variabilidad m√°s importante del dataset\n",
        "\n",
        "Variables seleccionadas:\n",
        "1. Age - Variable demogr√°fica continua, correlaci√≥n baja con otras variables\n",
        "2. Flight Distance - Variable operacional continua, informaci√≥n √∫nica sobre tipo de vuelo\n",
        "3. Seat comfort - Representa satisfacci√≥n con servicios f√≠sicos del avi√≥n\n",
        "4. On-board service - Representa satisfacci√≥n con servicios de atenci√≥n al cliente\n",
        "\n",
        "Justificaci√≥n de variables eliminadas:\n",
        "- Delays: Aunque importantes, tienen alta correlaci√≥n entre s√≠ (>0.9) y representan eventos externos\n",
        "- Otras variables de satisfacci√≥n: Muchas est√°n correlacionadas con las dos seleccionadas\n",
        "- Se mantiene diversidad: 2 continuas + 2 ordinales, cubriendo aspectos demogr√°ficos, operacionales y de satisfacci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "XsSsTo8AosPl",
        "outputId": "8d56f53e-d34f-4134-e18c-01db15735910"
      },
      "outputs": [],
      "source": [
        "# Selecci√≥n final de 4 variables basada en an√°lisis de correlaci√≥n\n",
        "selected_variables = ['Age', 'Flight Distance', 'Seat comfort', 'On-board service']\n",
        "\n",
        "# Crear dataset reducido\n",
        "df_reduced = df_num[selected_variables].copy()\n",
        "\n",
        "print(\"Variables seleccionadas para clustering:\")\n",
        "for i, var in enumerate(selected_variables, 1):\n",
        "    print(f\"{i}. {var}\")\n",
        "\n",
        "print(f\"\\nDimensiones del dataset reducido: {df_reduced.shape}\")\n",
        "\n",
        "# Verificar correlaciones entre variables seleccionadas\n",
        "print(\"\\nMatriz de correlaci√≥n de variables seleccionadas:\")\n",
        "corr_reduced = df_reduced.corr()\n",
        "print(corr_reduced.round(3))\n",
        "\n",
        "# Visualizar correlaci√≥n del subset seleccionado\n",
        "fig_reduced = ff.create_annotated_heatmap(\n",
        "    z=corr_reduced.values,\n",
        "    x=list(corr_reduced.columns),\n",
        "    y=list(corr_reduced.columns),\n",
        "    annotation_text=corr_reduced.round(2).values,\n",
        "    colorscale=\"RdBu\",\n",
        "    showscale=True\n",
        ")\n",
        "fig_reduced.update_layout(\n",
        "    title=\"Correlaci√≥n entre las 4 variables seleccionadas\",\n",
        "    title_x=0.5,\n",
        "    width=500,\n",
        "    height=500\n",
        ")\n",
        "fig_reduced.show()\n",
        "\n",
        "# Estad√≠sticas descriptivas del dataset reducido\n",
        "print(\"\\nEstad√≠sticas descriptivas de variables seleccionadas:\")\n",
        "print(df_reduced.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucboz5c3osPm"
      },
      "source": [
        "De esta forma se verifica lo que se estimaba y es posible trabajar con las nuevas variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4b6c047d994f40ea9e78a36a777042e0",
        "deepnote_cell_type": "markdown",
        "id": "PNGfTgtkrqC9"
      },
      "source": [
        "## 3. Preprocesamiento üé≠. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "713b3f0e61dd4841bb5b38c730d344d5",
        "deepnote_cell_type": "markdown",
        "id": "6RZD0fMNrqC-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media.tenor.com/R_WseIIwQ8QAAAAM/beavis-computer.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98400c7b5fec4af193eec3601f53891e",
        "deepnote_cell_type": "markdown",
        "id": "J6d4VEOTrqC-"
      },
      "source": [
        "Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.\n",
        "\n",
        "Bas√°ndose en los an√°lisis realizados anteriormente:\n",
        "1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]\n",
        "2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDSaGoq0OUp"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ad1e70818ad748638ca0927b07a76125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "deepnote_cell_type": "code",
        "id": "gBYG238wrqC-",
        "outputId": "fd6e6681-8f87-4fd1-c0c9-fcc66c875a31"
      },
      "outputs": [],
      "source": [
        "# Escriba su c√≥digo aqu√≠\n",
        "\n",
        "# Pipeline de preprocesamiento con escalado y PCA\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Escalado est√°ndar\n",
        "    ('pca', PCA(n_components=2))   # Reducci√≥n a 2 dimensiones\n",
        "])\n",
        "\n",
        "# Aplicar el pipeline a las variables seleccionadas\n",
        "data_pca_2d = preprocessing_pipeline.fit_transform(df_reduced)\n",
        "\n",
        "# Crear DataFrame con los componentes principales\n",
        "df_pca_2d = pd.DataFrame(data_pca_2d, columns=['PC1', 'PC2'])\n",
        "\n",
        "print(\"Pipeline de preprocesamiento creado exitosamente\")\n",
        "print(f\"Forma de datos originales: {df_reduced.shape}\")\n",
        "print(f\"Forma de datos transformados: {df_pca_2d.shape}\")\n",
        "\n",
        "# Informaci√≥n sobre la varianza explicada\n",
        "pca_model = preprocessing_pipeline.named_steps['pca']\n",
        "print(f\"\\nVarianza explicada por componente:\")\n",
        "print(f\"PC1: {pca_model.explained_variance_ratio_[0]:.3f} ({pca_model.explained_variance_ratio_[0]*100:.1f}%)\")\n",
        "print(f\"PC2: {pca_model.explained_variance_ratio_[1]:.3f} ({pca_model.explained_variance_ratio_[1]*100:.1f}%)\")\n",
        "print(f\"Varianza total explicada: {sum(pca_model.explained_variance_ratio_):.3f} ({sum(pca_model.explained_variance_ratio_)*100:.1f}%)\")\n",
        "\n",
        "# Visualizaci√≥n en 2D\n",
        "fig = px.scatter(\n",
        "    df_pca_2d,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    title='Datos proyectados con PCA (2 dimensiones)',\n",
        "    labels={'PC1': f'PC1 ({pca_model.explained_variance_ratio_[0]*100:.1f}% varianza)',\n",
        "            'PC2': f'PC2 ({pca_model.explained_variance_ratio_[1]*100:.1f}% varianza)'},\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(size=4, opacity=0.6, line=dict(width=0.5, color=\"white\"))\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_x=0.5,\n",
        "    plot_bgcolor=\"white\",\n",
        "    xaxis=dict(gridcolor=\"lightgray\", zerolinecolor=\"gray\"),\n",
        "    yaxis=dict(gridcolor=\"lightgray\", zerolinecolor=\"gray\")\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Tambi√©n crear pipeline para 3D para uso posterior\n",
        "preprocessing_pipeline_3d = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=3))\n",
        "])\n",
        "\n",
        "data_pca_3d = preprocessing_pipeline_3d.fit_transform(df_reduced)\n",
        "df_pca_3d = pd.DataFrame(data_pca_3d, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# Informaci√≥n sobre PCA 3D\n",
        "pca_3d_model = preprocessing_pipeline_3d.named_steps['pca']\n",
        "print(f\"\\nPCA 3D - Varianza explicada por componente:\")\n",
        "for i in range(3):\n",
        "    print(f\"PC{i+1}: {pca_3d_model.explained_variance_ratio_[i]:.3f} ({pca_3d_model.explained_variance_ratio_[i]*100:.1f}%)\")\n",
        "print(f\"Varianza total explicada (3D): {sum(pca_3d_model.explained_variance_ratio_):.3f} ({sum(pca_3d_model.explained_variance_ratio_)*100:.1f}%)\")\n",
        "\n",
        "# Visualizaci√≥n en 3D\n",
        "fig_3d = px.scatter_3d(\n",
        "    df_pca_3d,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    z='PC3',\n",
        "    title='Datos proyectados con PCA (3 dimensiones)',\n",
        "    labels={'PC1': f'PC1 ({pca_3d_model.explained_variance_ratio_[0]*100:.1f}%)',\n",
        "            'PC2': f'PC2 ({pca_3d_model.explained_variance_ratio_[1]*100:.1f}%)',\n",
        "            'PC3': f'PC3 ({pca_3d_model.explained_variance_ratio_[2]*100:.1f}%)'},\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig_3d.update_traces(\n",
        "    marker=dict(size=3, opacity=0.6)\n",
        ")\n",
        "\n",
        "fig_3d.update_layout(title_x=0.5)\n",
        "fig_3d.show()\n",
        "\n",
        "# An√°lisis de componentes principales\n",
        "print(\"\\nContribuciones de variables originales a los componentes principales:\")\n",
        "feature_names = df_reduced.columns\n",
        "components_df = pd.DataFrame(\n",
        "    pca_model.components_.T,\n",
        "    columns=['PC1', 'PC2'],\n",
        "    index=feature_names\n",
        ")\n",
        "print(components_df.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uczt61M9osPz",
        "outputId": "cd81a507-8453-4aaa-ae7d-ca13f6be0f3d"
      },
      "outputs": [],
      "source": [
        "# Inicializar el escalador\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Ajustar el escalador a los datos y transformarlos.\n",
        "# Esto calcula el m√≠nimo y el m√°ximo de cada columna y luego los escala.\n",
        "df_reduced = pd.DataFrame(scaler.fit_transform(df_reduced), columns=df_reduced.columns)\n",
        "\n",
        "# Imprimir las primeras filas del DataFrame normalizado para verificar\n",
        "print(\"Primeras filas del DataFrame normalizado (entre 0 y 1):\")\n",
        "print(df_reduced.head())\n",
        "\n",
        "# Imprimir estad√≠sticas resumidas para verificar la normalizaci√≥n\n",
        "print(\"\\nEstad√≠sticas resumidas del DataFrame normalizado:\")\n",
        "print(df_reduced.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hILiJHYosPz"
      },
      "source": [
        "El pipeline de preprocesamiento implementado incluye escalado est√°ndar que se ha discutido anteriormente su uso seguido de PCA.\n",
        "\n",
        "Proyecci√≥n 2D: Los dos primeros componentes principales capturan aproximadamente el 60% de la varianza total de los datos, lo que es razonable para una reducci√≥n dimensional significativa. En la visualizaci√≥n 2D se observa una distribuci√≥n dispersa de los puntos, con algunas concentraciones que sugieren la presencia de grupos naturales en los datos. La proyecci√≥n revela patrones que no eran evidentes en las variables originales, mostrando c√≥mo las combinaciones lineales de las 4 variables escogidas (Age, Flight Distance, Seat comfort, On-board service) se relacionan entre s√≠.\n",
        "\n",
        "Contribuciones de las variables: PC1 parece estar dominado por las variables de satisfacci√≥n (Seat comfort y On-board service), PC2 muestra mayor influencia de las variables demogr√°ficas y operacionales (Age y Flight Distance)\n",
        "\n",
        "Es importante notar que para realizar una comparaci√≥n, se realiz√≥ una proyecci√≥n 3D, la cual permite capturar m√°s varianza y proporciona una perspectiva m√°s completa de la estructura de los datos, facilitando la identificaci√≥n de clusters tridimensionales que no son visibles en 2D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd281470d3054764a63d857cfa7d52a6",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "7ENoOtIIrqC_"
      },
      "source": [
        "## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db89e9c9f35c44abbd8991180226c0ea",
        "deepnote_cell_type": "markdown",
        "id": "fbGw6Sa-rqC_"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif\" width=250>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3e2f59fa12954641af7a854a4e203694",
        "deepnote_cell_type": "markdown",
        "id": "nl_ccu9brqDA"
      },
      "source": [
        "Con el objetivo de mantener la claridad en su an√°lisis, Don Mathias le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.\n",
        "\n",
        "1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]\n",
        "\n",
        "2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]\n",
        "\n",
        "3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CKJz-ssSosP1",
        "outputId": "6646922a-0d59-4d6d-c31c-b69d8ed7a37b"
      },
      "outputs": [],
      "source": [
        "df_reduced.head() #veamos que se ve todo bien para trabajar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cS1FR00NlF"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "be86896911244aa89e3b5f3f00a286af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "deepnote_cell_type": "code",
        "id": "iaPZFmjyrqDA",
        "outputId": "4f9ddcba-6859-4e54-a5d5-a8cc82ea2a0e"
      },
      "outputs": [],
      "source": [
        "# Pipeline completo para detecci√≥n de anomal√≠as 2D\n",
        "# Incluye escalado est√°ndar + PCA + detecci√≥n de outliers en una sola \"tuber√≠a\"\n",
        "outlier_detection_pipeline_2d = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Escalado est√°ndar para normalizar variables, sino los outliers los determinar√° la coliumna con mayor escala\n",
        "    ('pca', PCA(n_components=2)),  # Reducci√≥n a 2 dimensiones principales\n",
        "    ('isolation_forest', IsolationForest(contamination=0.01, random_state=42))  # Detecci√≥n de 1% outliers\n",
        "])\n",
        "\n",
        "# Pipeline completo para detecci√≥n de anomal√≠as 3D\n",
        "# Misma estructura pero manteniendo 3 componentes principales para mayor informaci√≥n\n",
        "outlier_detection_pipeline_3d = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Escalado est√°ndar para normalizar variables\n",
        "    ('pca', PCA(n_components=3)),  # Reducci√≥n a 3 dimensiones principales\n",
        "    ('isolation_forest', IsolationForest(contamination=0.01, random_state=42))  # Detecci√≥n de 1% outliers\n",
        "])\n",
        "\n",
        "# Aplicar pipelines completos a los datos reducidos\n",
        "# Cada pipeline procesa desde datos originales hasta detecci√≥n final\n",
        "anomaly_labels_2d = outlier_detection_pipeline_2d.fit_predict(df_reduced)\n",
        "anomaly_labels_3d = outlier_detection_pipeline_3d.fit_predict(df_reduced)\n",
        "\n",
        "# Crear pipelines auxiliares para obtener coordenadas PCA para visualizaci√≥n\n",
        "# Necesitamos las coordenadas transformadas para graficar, no solo las etiquetas de anomal√≠as\n",
        "scaler_pca_2d = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Mismo escalado que pipeline principal\n",
        "    ('pca', PCA(n_components=2))   # Misma reducci√≥n PCA 2D\n",
        "])\n",
        "\n",
        "scaler_pca_3d = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Mismo escalado que pipeline principal\n",
        "    ('pca', PCA(n_components=3))   # Misma reducci√≥n PCA 3D\n",
        "])\n",
        "\n",
        "# Obtener coordenadas PCA transformadas para visualizaci√≥n\n",
        "data_pca_2d_outliers = scaler_pca_2d.fit_transform(df_reduced)\n",
        "df_pca_2d_outliers = pd.DataFrame(data_pca_2d_outliers, columns=['PC1', 'PC2'])\n",
        "\n",
        "data_pca_3d_outliers = scaler_pca_3d.fit_transform(df_reduced)\n",
        "df_pca_3d_outliers = pd.DataFrame(data_pca_3d_outliers, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# Obtener informaci√≥n de varianza explicada de los pasos PCA\n",
        "pca_2d_step = scaler_pca_2d.named_steps['pca']\n",
        "pca_3d_step = scaler_pca_3d.named_steps['pca']\n",
        "\n",
        "print(\"Pipeline de detecci√≥n de anomal√≠as creado exitosamente\")\n",
        "print(f\"Pipeline 2D: StandardScaler -> PCA(2D) -> IsolationForest\")\n",
        "print(f\"Pipeline 3D: StandardScaler -> PCA(3D) -> IsolationForest\")\n",
        "print(f\"Forma de datos originales: {df_reduced.shape}\")\n",
        "print(f\"Anomal√≠as detectadas con pipeline 2D: {np.sum(anomaly_labels_2d == -1)}\")\n",
        "print(f\"Anomal√≠as detectadas con pipeline 3D: {np.sum(anomaly_labels_3d == -1)}\")\n",
        "\n",
        "### Aqu√≠ comienzan los gr√°ficos\n",
        "\n",
        "# Convertir labels de anomal√≠as a texto legible para visualizaci√≥n\n",
        "# IsolationForest devuelve: 1 = normal, -1 = anomal√≠a\n",
        "anomaly_binary_2d = np.where(anomaly_labels_2d == 1, 'Normal', 'Anomaly')\n",
        "anomaly_binary_3d = np.where(anomaly_labels_3d == 1, 'Normal', 'Anomaly')\n",
        "\n",
        "# Visualizaci√≥n 2D con detecci√≥n de anomal√≠as\n",
        "# Usamos las coordenadas PCA transformadas y coloreamos por tipo de anomal√≠a\n",
        "fig_2d = px.scatter(\n",
        "    x=df_pca_2d_outliers['PC1'],\n",
        "    y=df_pca_2d_outliers['PC2'],\n",
        "    color=anomaly_binary_2d,\n",
        "    title='Detecci√≥n de Anomal√≠as con Isolation Forest (2D)',\n",
        "    labels={\n",
        "        'x': f'PC1 ({pca_2d_step.explained_variance_ratio_[0]*100:.1f}% varianza)',\n",
        "        'y': f'PC2 ({pca_2d_step.explained_variance_ratio_[1]*100:.1f}% varianza)',\n",
        "        'color': 'Clase'\n",
        "    },\n",
        "    color_discrete_map={\n",
        "        'Normal': 'blue',    # Puntos normales en azul\n",
        "        'Anomaly': 'red'     # Outliers en rojo para destacar\n",
        "    },\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "# Configurar estilo del gr√°fico 2D para consistencia con gr√°ficos anteriores\n",
        "fig_2d.update_traces(marker=dict(size=4, opacity=0.6))\n",
        "fig_2d.update_layout(\n",
        "    title_x=0.5,\n",
        "    plot_bgcolor='white',\n",
        "    xaxis=dict(gridcolor=\"lightgray\", zerolinecolor=\"gray\"),\n",
        "    yaxis=dict(gridcolor=\"lightgray\", zerolinecolor=\"gray\")\n",
        ")\n",
        "\n",
        "fig_2d.show()\n",
        "\n",
        "# Visualizaci√≥n 3D con detecci√≥n de anomal√≠as\n",
        "# Gr√°fico interactivo 3D coloreado por tipo de anomal√≠a\n",
        "fig_3d = px.scatter_3d(\n",
        "    x=df_pca_3d_outliers['PC1'],\n",
        "    y=df_pca_3d_outliers['PC2'],\n",
        "    z=df_pca_3d_outliers['PC3'],\n",
        "    color=anomaly_binary_3d,\n",
        "    title='Detecci√≥n de Anomal√≠as con Isolation Forest (3D)',\n",
        "    labels={\n",
        "        'x': f'PC1 ({pca_3d_step.explained_variance_ratio_[0]*100:.1f}%)',\n",
        "        'y': f'PC2 ({pca_3d_step.explained_variance_ratio_[1]*100:.1f}%)',\n",
        "        'z': f'PC3 ({pca_3d_step.explained_variance_ratio_[2]*100:.1f}%)',\n",
        "        'color': 'Clase'\n",
        "    },\n",
        "    color_discrete_map={\n",
        "        'Normal': 'blue',    # Puntos normales en azul\n",
        "        'Anomaly': 'red'     # Outliers en rojo para destacar\n",
        "    },\n",
        "    width=800,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "# Configurar estilo del gr√°fico 3D\n",
        "fig_3d.update_traces(marker=dict(size=3, opacity=0.6))\n",
        "fig_3d.update_layout(title_x=0.5)\n",
        "fig_3d.show()\n",
        "\n",
        "# Estad√≠sticas detalladas de detecci√≥n de anomal√≠as\n",
        "anomaly_count_2d = np.sum(anomaly_labels_2d == -1)  # Contar etiquetas -1 (anomal√≠as)\n",
        "anomaly_count_3d = np.sum(anomaly_labels_3d == -1)\n",
        "total_points = len(anomaly_labels_2d)\n",
        "\n",
        "print(f\"\\nEstad√≠sticas de Detecci√≥n de Anomal√≠as:\")\n",
        "print(f\"Total de puntos: {total_points}\")\n",
        "print(f\"Anomal√≠as detectadas (2D): {anomaly_count_2d} ({(anomaly_count_2d/total_points)*100:.1f}%)\")\n",
        "print(f\"Anomal√≠as detectadas (3D): {anomaly_count_3d} ({(anomaly_count_3d/total_points)*100:.1f}%)\")\n",
        "\n",
        "# Comparaci√≥n de anomal√≠as entre espacios 2D y 3D\n",
        "# Verificar cu√°ntas anomal√≠as son comunes en ambas dimensionalidades\n",
        "common_anomalies = np.sum((anomaly_labels_2d == -1) & (anomaly_labels_3d == -1))\n",
        "print(f\"Anomal√≠as comunes entre 2D y 3D: {common_anomalies}\")\n",
        "\n",
        "# Mostrar √≠ndices espec√≠ficos de anomal√≠as para an√°lisis posterior\n",
        "print(f\"\\nPrimeras 5 anomal√≠as detectadas en 2D:\")\n",
        "anomaly_indices_2d = np.where(anomaly_labels_2d == -1)[0][:5]\n",
        "print(f\"√çndices: {anomaly_indices_2d}\")\n",
        "\n",
        "print(f\"\\nPrimeras 5 anomal√≠as detectadas en 3D:\")\n",
        "anomaly_indices_3d = np.where(anomaly_labels_3d == -1)[0][:5]\n",
        "print(f\"√çndices: {anomaly_indices_3d}\")\n",
        "\n",
        "# Resumen de pipelines utilizados con informaci√≥n t√©cnica detallada\n",
        "print(f\"\\nResumen de Pipelines de Detecci√≥n de Anomal√≠as:\")\n",
        "print(f\"Pipeline 2D: StandardScaler -> PCA(2 componentes) -> IsolationForest\")\n",
        "print(f\"- Varianza explicada total: {pca_2d_step.explained_variance_ratio_.sum()*100:.1f}%\")\n",
        "print(f\"- Distribuci√≥n por componente: PC1({pca_2d_step.explained_variance_ratio_[0]*100:.1f}%), PC2({pca_2d_step.explained_variance_ratio_[1]*100:.1f}%)\")\n",
        "print(f\"Pipeline 3D: StandardScaler -> PCA(3 componentes) -> IsolationForest\")\n",
        "print(f\"- Varianza explicada total: {pca_3d_step.explained_variance_ratio_.sum()*100:.1f}%\")\n",
        "print(f\"- Distribuci√≥n por componente: PC1({pca_3d_step.explained_variance_ratio_[0]*100:.1f}%), PC2({pca_3d_step.explained_variance_ratio_[1]*100:.1f}%), PC3({pca_3d_step.explained_variance_ratio_[2]*100:.1f}%)\")\n",
        "print(f\"- Par√°metro de contaminaci√≥n: {0.01*100}% (seg√∫n especificaci√≥n del enunciado)\")\n",
        "print(f\"- Random state: 42 (para reproducibilidad de resultados)\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n Eliminando outliers y creando dfmax\")\n",
        "\n",
        "# Tomamos el pipeline 3D ya que retiene m√°s informaci√≥n para el an√°lisis final\n",
        "# La l√≥gica es la misma para el pipeline 2D, pero 3D es generalmente preferible si la varianza explicada lo justifica.\n",
        "final_pipeline = outlier_detection_pipeline_3d\n",
        "\n",
        "# Obtener las etiquetas de anomal√≠as del pipeline final (etiquetas 1 y -1)\n",
        "anomaly_labels_final = final_pipeline.fit_predict(df_reduced)\n",
        "\n",
        "# Filtrar df_reduced para mantener solo los puntos que no son anomal√≠as\n",
        "# El valor 1 significa que la muestra es un \"inlier\" (no es un outlier)\n",
        "dfmax_filtered = df_reduced[anomaly_labels_final == 1]\n",
        "\n",
        "# Obtener las columnas que el pipeline utiliza\n",
        "# Esto se basa en el 'feature_names_in_' del scaler\n",
        "cols_del_pipeline = final_pipeline.named_steps['scaler'].feature_names_in_\n",
        "\n",
        "# Crear el DataFrame final dfmax solo con las columnas del pipeline para que la siguiente parte no tarde mucho tiempo\n",
        "dfmax = dfmax_filtered[cols_del_pipeline]\n",
        "\n",
        "print(f\"Shape de df_reduced original: {df_reduced.shape}\")\n",
        "print(f\"Shape del nuevo DataFrame dfmax (sin outliers): {dfmax.shape}\")\n",
        "print(f\"N√∫mero de outliers eliminados: {df_reduced.shape[0] - dfmax.shape[0]}\")\n",
        "print(f\"Columnas en dfmax: {list(dfmax.columns)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9rCOTphosP2"
      },
      "source": [
        "Respondiendo a \"¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as?\", el rendimiento del modelo es bueno ya que cumple con el par√°metro de reconocer el 1%, detectando exactamente 2362 anomal√≠as en ambas dimensionalidades de un total de 238114 puntos. Sin embargo, presenta limitaciones importantes, primero, solo 602 anomal√≠as son comunes entre los espacios 2D y 3D (25% de solapamiento), indicando inconsistencias significativas seg√∫n la dimensionalidad utilizada.\n",
        "La visualizaci√≥n muestra que IsolationForest identifica correctamente puntos en regiones perif√©ricas como anomal√≠as, pero la dispersi√≥n de outliers sugiere que podr√≠a estar capturando tanto anomal√≠as genuinas como variaciones naturales extremas. La diferencia entre detecciones 2D y 3D indica que la tercera componente principal aporta informaci√≥n valiosa no visible en el espacio bidimensional.\n",
        "Para una evaluaci√≥n robusta, se requieren m√©tricas cuantitativas adicionales como coeficiente de silhouette y validaci√≥n humana con expertos en la tarea para confirmar si las anomal√≠as detectadas son verdaderamente significativas en el contexto de satisfacci√≥n de aerol√≠neas. El modelo funciona como herramienta exploratoria eficaz, pero necesita refinamiento para garantizar la detecci√≥n de outliers genuinos. Tambi√©n se podr√≠an buscar hiperparametros para determinar la dimensionalidad √≥ptima respecto a las dimensiones y hallar mejor los outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rJCnc43iosP3",
        "outputId": "68187b6a-fafc-478c-9146-f951d1eb20cf"
      },
      "outputs": [],
      "source": [
        "dfmax.head()  # Mostrar las primeras filas de dfmax para verificar su contenido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3871e2fe5bdd422dbdbfaebf75503ae3",
        "deepnote_cell_type": "markdown",
        "id": "zQFTklmVrqDB"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "236333de6dd445c182aefcc507589325",
        "deepnote_cell_type": "markdown",
        "id": "YpNj4wbPrqDB"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.imgflip.com/6xz0ij.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7e1ceb91be94b1da2ab8be97dfac999",
        "deepnote_cell_type": "markdown",
        "id": "CR3hzRxrrqDB"
      },
      "source": [
        "Motivado por incrementar su fortuna, Don Mathias le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:\n",
        "\n",
        "1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]\n",
        "2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]\n",
        "\n",
        "> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_T_zTg0MXB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoHyKtaTosP5",
        "outputId": "0a971867-14f8-4638-bf6b-3919f3e37634"
      },
      "outputs": [],
      "source": [
        "# Diccionarios para almacenar los valores de AIC y BIC para cada n√∫mero de cl√∫sters\n",
        "aic = {}\n",
        "bic = {}\n",
        "\n",
        "# Rango de cl√∫sters a explorar seg√∫n la solicitud (de 3 a 8)\n",
        "n_clusters_range = range(3, 9)\n",
        "\n",
        "# Bucle para entrenar el modelo con diferentes n√∫meros de cl√∫sters\n",
        "for n_clusters in n_clusters_range:\n",
        "    # 1. Crear un pipeline para la normalizaci√≥n y el GMM\n",
        "    gmm_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('gmm', GaussianMixture(n_components=n_clusters, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # 2. Entrenar el pipeline con el DataFrame dfmax\n",
        "    gmm_pipeline.fit(dfmax)\n",
        "\n",
        "    # 3. Almacenar los valores de AIC y BIC\n",
        "    aic[n_clusters] = gmm_pipeline.named_steps['gmm'].aic(dfmax)\n",
        "    bic[n_clusters] = gmm_pipeline.named_steps['gmm'].bic(dfmax)\n",
        "\n",
        "print(\"An√°lisis de cl√∫sters completado.\")\n",
        "\n",
        "# Crear un DataFrame con los resultados\n",
        "results_df = pd.DataFrame({\n",
        "    'N√∫mero de Cl√∫sters': aic.keys(),\n",
        "    'AIC': aic.values(),\n",
        "    'BIC': bic.values()\n",
        "})\n",
        "\n",
        "# Formatear las columnas num√©ricas para una mejor visualizaci√≥n\n",
        "results_df['AIC'] = results_df['AIC'].map('{:.2f}'.format)\n",
        "results_df['BIC'] = results_df['BIC'].map('{:.2f}'.format)\n",
        "\n",
        "# Imprimir la tabla\n",
        "print(\"\\nResultados de AIC y BIC por n√∫mero de cl√∫sters:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrHTQceqosP6"
      },
      "source": [
        "El n√∫mero √≥ptimo de cl√∫sters es el que minimiza el AIC y el BIC, ya que ambos criterios equilibran el ajuste del modelo y su complejidad. En t√©rminos estad√≠sticos, estos criterios son una estimaci√≥n de la p√©rdida de informaci√≥n cuando un modelo de agrupamiento se utiliza para representar el proceso que gener√≥ los datos.\n",
        "\n",
        "El AIC estima la calidad del modelo en relaci√≥n con otros modelos. Un valor m√°s bajo indica que el modelo es mejor para explicar los datos con un menor n√∫mero de par√°metros.\n",
        "\n",
        "El BIC es similar, pero aplica una penalizaci√≥n m√°s fuerte por el n√∫mero de cl√∫sters ya que agega un t√©rmino nuevo. Esto lo hace √∫til para seleccionar modelos m√°s simples, lo cual es ideal para evitar el sobreajuste y para asegurar que la segmentaci√≥n resultante sea m√°s interpretable y √∫til para el negocio.\n",
        "\n",
        "Al penalizar modelos m√°s complejos, estos indicadores ayudan a evitar el sobreajuste, favoreciendo un n√∫mero de cl√∫sters que capture adecuadamente la estructura de los datos sin ser innecesariamente complejo.\n",
        "\n",
        "Con base en nuestros resultados, es f√°cil ver que el n√∫mero √≥ptimo de cl√∫sters es 6, ya que tiene valores de AIC y BIC significativamente m√°s bajos que los modelos con otras cantidades de cl√∫sters, lo que indica un mejor equilibrio entre el ajuste y la complejidad del modelo. Aprovechemos de verlo gr√°ficamente en la siguiente celda, donde podemos veriticar que el m√≠nimo de ambos es en 6 clusters.\n",
        "\n",
        "Por qu√© ambas curvas son id√©nticas? debido a que los modelos tienen el mismo n√∫mero de par√°metros y debemos recordar de que AIC y BIC se calculan de la misma manera pero BIC penaliza el n√∫mero de par√°metros, en este caso como tenemos el mismo n√∫mero de par√°metros, AIC y BIC son iguales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7b1HXW5EosP6",
        "outputId": "ca9f9efc-2da0-401a-83c3-033a471cfe07"
      },
      "outputs": [],
      "source": [
        "# Obtener los valores de los diccionarios, tratamos con arreglos np pero no nos resulto bien\n",
        "n_clusters_range = list(aic.keys())\n",
        "aic_values = list(aic.values())\n",
        "bic_values = list(bic.values())\n",
        "\n",
        "# Gr√°fico para visualizar AIC y BIC\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_clusters_range, aic_values, marker='o', label='AIC')\n",
        "plt.plot(n_clusters_range, bic_values, marker='o', label='BIC')\n",
        "plt.xlabel('N√∫mero de Cl√∫sters')\n",
        "plt.ylabel('Valor del Criterio')\n",
        "plt.title('Criterios AIC y BIC para la Selecci√≥n de Cl√∫sters')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLa justificaci√≥n visual se basa en el punto m√°s bajo de las curvas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dd342e336254418ba766b29dce16b267",
        "deepnote_cell_type": "markdown",
        "id": "P9CERnaerqDC"
      },
      "source": [
        "## 6. An√°lisis de resultados üìä [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "953b5ad01a704b50b899db7176d1b7b2",
        "deepnote_cell_type": "markdown",
        "id": "I1yNa111rqDC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5b/03/4e/5b034e96d84c6c6b57a9a04ca14aac02.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd90e2f135404353ac0b5ab844936ca7",
        "deepnote_cell_type": "markdown",
        "id": "dg0Qx4RZrqDC"
      },
      "source": [
        "Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:\n",
        "\n",
        "1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]\n",
        "\n",
        "2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]\n",
        "\n",
        "3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]\n",
        "\n",
        "4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]\n",
        "\n",
        "5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRN0zZip0IMB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9abf4dbc643e40cebe99fcb1ff3ff413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "deepnote_cell_type": "code",
        "id": "XmZrz15GrqDC",
        "outputId": "e304264c-7694-4bb4-dde7-674ebe6dafbe"
      },
      "outputs": [],
      "source": [
        "# 1. Re-entrenar el pipeline con el n√∫mero √≥ptimo de cl√∫sters (6)\n",
        "# El 'fit_predict' asignar√° a cada fila el cl√∫ster al que pertenece\n",
        "n_clusters_optimo = 6\n",
        "\n",
        "gmm_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gmm', GaussianMixture(n_components=n_clusters_optimo, random_state=42))\n",
        "])\n",
        "\n",
        "cluster_labels = gmm_pipeline.fit_predict(dfmax)\n",
        "\n",
        "# Agregar las etiquetas de los cl√∫sters al DataFrame\n",
        "df_clustered = dfmax.copy()\n",
        "df_clustered['cluster'] = cluster_labels\n",
        "\n",
        "# 2. Reducci√≥n de dimensionalidad para visualizaci√≥n en 2D y 3D\n",
        "# Es necesario usar PCA para reducir los datos a 2 y 3 dimensiones\n",
        "pca_2d = PCA(n_components=2)\n",
        "pca_3d = PCA(n_components=3)\n",
        "\n",
        "# Aplicar PCA y agregar las nuevas dimensiones al DataFrame\n",
        "pca_2d_results = pca_2d.fit_transform(df_clustered.drop('cluster', axis=1))\n",
        "df_clustered['PC1_2d'] = pca_2d_results[:, 0]\n",
        "df_clustered['PC2_2d'] = pca_2d_results[:, 1]\n",
        "\n",
        "pca_3d_results = pca_3d.fit_transform(df_clustered.drop('cluster', axis=1))\n",
        "df_clustered['PC1_3d'] = pca_3d_results[:, 0]\n",
        "df_clustered['PC2_3d'] = pca_3d_results[:, 1]\n",
        "df_clustered['PC3_3d'] = pca_3d_results[:, 2]\n",
        "\n",
        "# 3. Visualizar los cl√∫sters en 2D\n",
        "fig_2d = px.scatter(\n",
        "    df_clustered,\n",
        "    x='PC1_2d',\n",
        "    y='PC2_2d',\n",
        "    color='cluster',\n",
        "    title='Visualizaci√≥n de Cl√∫sters en 2D',\n",
        "    labels={'PC1_2d': 'PC1', 'PC2_2d': 'PC2', 'cluster': 'Cl√∫ster'}\n",
        ")\n",
        "fig_2d.update_layout(title_x=0.5)\n",
        "fig_2d.show()\n",
        "\n",
        "# 4. Visualizar los cl√∫sters en 3D\n",
        "fig_3d = px.scatter_3d(\n",
        "    df_clustered,\n",
        "    x='PC1_3d',\n",
        "    y='PC2_3d',\n",
        "    z='PC3_3d',\n",
        "    color='cluster',\n",
        "    title='Visualizaci√≥n de Cl√∫sters en 3D',\n",
        "    labels={'PC1_3d': 'PC1', 'PC2_3d': 'PC2', 'PC3_3d': 'PC3', 'cluster': 'Cl√∫ster'}\n",
        ")\n",
        "fig_3d.update_layout(title_x=0.5)\n",
        "fig_3d.show()\n",
        "\n",
        "# 5. Estad√≠sticas descriptivas de cada cl√∫ster\n",
        "print(\"\\n Estad√≠sticas Descriptivas por Cl√∫ster\")\n",
        "cluster_summary = df_clustered.groupby('cluster').agg(['mean', 'std'])\n",
        "print(cluster_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi7vHabhosP9"
      },
      "source": [
        "No es posible distinguir claramente entre los cl√∫sters generados en una proyecci√≥n bidimensional. Aunque los cl√∫sters revelan patrones de agrupaci√≥n (se pueden ver patrones de variaciones en el espacio del eje y con tendencias similares en los clusters), no est√°n perfectamente separados. La superposici√≥n entre ellos es evidente, lo que indica que, si bien el modelo ha encontrado una estructura subyacente en los espacio de los datos, esta no es linealmente separable en dos dimensiones, por ejemplo un svm tendr√≠a muchos problemas en este caso. Es probable que la estructura real de los datos solo pueda ser capturada en una dimensi√≥n superior, lo que hace que los cl√∫sters sean m√°s reconocibles en un espacio tridimensional como veremos a continuaci√≥n.\n",
        "\n",
        "\n",
        "Al ver los clusters en 3 dimensiones se puede ver la separabilidad en los patrones reconocidos anteriormente, a pesar de no ser tan perfecta, logran apreciarse bien los clusters\n",
        "\n",
        "\n",
        "La visualizaci√≥n en 3D refuerza y valida por completo las decisiones de haber eliminado los outliers, normalizar los datos y haber elegido 6 cl√∫steres como el n√∫mero √≥ptimo. Esto se debe a que el gr√°fico tridimensional ofrece una perspectiva m√°s detallada que demuestra la efectividad de los pasos de preprocesamiento en comparaci√≥n a sino hubiesemos tomado esas medidas, de partida sino normalizabamos, los clusters hubiesen sido dominados por las variables de mayor esacala.\n",
        "\n",
        "La capacidad de ver los cl√∫steres separados, aunque con cierto solapamiento, en tres dimensiones confirma que el haber eliminado los datos at√≠picos fue una decisi√≥n crucial. Al enfocarse en la estructura principal de los datos, se evit√≥ que los cl√∫steres se distorsionaran por outliers, lo que permiti√≥ una agrupaci√≥n m√°s coherente. incluso la normalizaci√≥n fue clave, ya que si los datos no se hubieran escalado, las variables con rangos m√°s amplios habr√≠an dominado la formaci√≥n de los cl√∫steres, sesgando los resultados. El hecho de que las agrupaciones sean distintivas demuestra que la normalizaci√≥n contribuy√≥ a una segmentaci√≥n equitativa y significativa.\n",
        "\n",
        "El resultado final tambi√©n justifica la elecci√≥n de los 6 cl√∫steres basada en los criterios de AIC y BIC, adem√°s de que se separan muy bien los clusters. Mientras que en la proyecci√≥n en 2D el solapamiento era significativo, la visualizaci√≥n en 3D revela una separaci√≥n m√°s clara que corrobora la elecci√≥n estad√≠stica. Este ligero solapamiento en tres dimensiones es un resultado esperado en datos reales, lo que indica que el modelo no ha sobreajustado los datos, sino que ha capturado la estructura intr√≠nseca de la informaci√≥n de manera eficiente y realista.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8igIgDwpq9mG"
      },
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\" width=300>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7cb425aec99b4079954fd707109c42c3",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-26T06:15:51.197Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
